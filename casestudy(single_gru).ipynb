{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "272h1ufI6HS2"
      },
      "source": [
        "\n",
        "# **Neural machine translation with attention**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RuBjS6ebz_pL",
        "outputId": "0d95ff8e-d595-4ddb-8d65-bc0ebbd142f3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XOmKqHsl7dhS"
      },
      "source": [
        "This notebook trains a sequence to sequence (seq2seq) model for spoken sentence to sign sentence translation. This is an advanced example that assumes some knowledge of sequence to sequence models.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        },
        "id": "H7yEo84K6-sZ",
        "outputId": "c862d589-2675-4a64-ff8d-2646561ba434"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/bin/sh: 1: nvidia-smi: not found\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<h1>DONT PROCEED</h1>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from IPython.display import HTML\n",
        "from subprocess import getoutput\n",
        "s = getoutput('nvidia-smi')\n",
        "print(s)\n",
        "if 'K80' in s:\n",
        "    gpu = 'K80'\n",
        "elif 'T4' in s:\n",
        "    gpu = 'T4'\n",
        "elif 'P100' in s:\n",
        "    gpu = 'P100'\n",
        "else:\n",
        "    gpu='DONT PROCEED'\n",
        "display(HTML(f\"<h1>{gpu}</h1>\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GzASbFhOKq2W",
        "outputId": "a6b19d01-68f1-4c33-d49b-6d102a7a395c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting chart-studio\n",
            "  Downloading chart_studio-1.1.0-py3-none-any.whl (64 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/64.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.4/64.4 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: plotly in /usr/local/lib/python3.10/dist-packages (from chart-studio) (5.15.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from chart-studio) (2.31.0)\n",
            "Collecting retrying>=1.3.3 (from chart-studio)\n",
            "  Downloading retrying-1.3.4-py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from chart-studio) (1.16.0)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly->chart-studio) (8.2.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from plotly->chart-studio) (24.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->chart-studio) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->chart-studio) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->chart-studio) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->chart-studio) (2024.2.2)\n",
            "Installing collected packages: retrying, chart-studio\n",
            "Successfully installed chart-studio-1.1.0 retrying-1.3.4\n"
          ]
        }
      ],
      "source": [
        "!pip install chart-studio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "clt4aHjH5X3G",
        "outputId": "62880367-0fe7-4356-c36f-7283fc0fdfdb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Colab only includes TensorFlow 2.x; %tensorflow_version has no effect.\n"
          ]
        }
      ],
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "import tensorflow as tf\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import unicodedata\n",
        "import re\n",
        "import numpy as np\n",
        "import os\n",
        "import io\n",
        "import time\n",
        "import string\n",
        "from string import digits\n",
        "\n",
        "import chart_studio.plotly\n",
        "import chart_studio.plotly as py\n",
        "from plotly.offline import init_notebook_mode, iplot\n",
        "#%plotly.offline.init_notebook_mode(connected=True)\n",
        "import plotly.graph_objs as go"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j9BfTUgb73V0"
      },
      "source": [
        "##**Download and prepare the dataset**\n",
        "\n",
        "\n",
        "1. Add a start and end token to each sentence.\n",
        "2. Clean the sentences by removing special characters.\n",
        "3. Create a word index and reverse word index (dictionaries mapping from word → id and id → word).\n",
        "4. Pad each sentence to a maximum length."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "ybzRHiHcAoGL"
      },
      "outputs": [],
      "source": [
        "#file_path = '/content/ISL Corpus sign glosses.csv'\n",
        "\n",
        "# import csv\n",
        "\n",
        "# csv_file_path = '/content/ISL Corpus sign glosses.csv'\n",
        "txt_file_path = 'example.txt'\n",
        "\n",
        "# with open(csv_file_path, 'r') as csv_file:\n",
        "#     # Assuming the CSV file has a header\n",
        "#     csv_reader = csv.reader(csv_file)\n",
        "#     header = next(csv_reader)\n",
        "\n",
        "#     with open(txt_file_path, 'w') as txt_file:\n",
        "#         # Write header to the text file\n",
        "#         txt_file.write('\\t'.join(header) + '\\n')\n",
        "\n",
        "#         # Write data to the text file\n",
        "#         for row in csv_reader:\n",
        "#             txt_file.write('\\t'.join(row) + '\\n')\n",
        "\n",
        "# print(f'Conversion complete. Text file saved at: {txt_file_path}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oZ2KBuMDAzZd",
        "outputId": "4b8e3707-5268-4ade-9a42-8d087c16c243"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['are you free today\\tYOU FREE TODAY',\n",
              " 'are you hiding something\\tYOU HIDE SOMETHING',\n",
              " 'bring water for me\\tBRING WATER ME',\n",
              " 'can i help you\\tI HELP YOU',\n",
              " 'can you repeat that please\\tYOU REPEAT PLEASE',\n",
              " 'comb your hair\\tCOMB YOU HAIR',\n",
              " 'congratulations\\tCONGRATULATIIONS',\n",
              " 'could you please talk slower\\tYOU PLEASE TALK SLOWER',\n",
              " 'do me a favour\\tDO ME FAVOUR',\n",
              " 'do not abuse him\\tDONOT ABUSE HIM',\n",
              " 'do not be stubborn\\tDONOT BE STUBBORN',\n",
              " 'do not hurt me \\tDONOT HURT ME',\n",
              " 'do not make me angry\\tDO NOT MAKE ME ANGRY',\n",
              " 'do not take it to the heart\\tDO NOT TAKE IT HEART',\n",
              " 'do not worry\\tDO NOT WORRY',\n",
              " 'do you need something\\tDO YOU NEED SOMETHING',\n",
              " 'go and sleep\\tGO SLEEP',\n",
              " 'had your food\\tYOUR FOOD',\n",
              " 'he came by train\\tHE CAME TRAIN',\n",
              " 'He is going into the room\\tHE GO  INTO  ROOM',\n",
              " 'he is on the way\\tHE ON WAY',\n",
              " 'he she is my friend\\tHE SHE MY FRIEND',\n",
              " 'he would be coming today\\tHE COMING TODAY',\n",
              " 'help me\\tHELP ME',\n",
              " 'hi how are you\\tHI HOW YOU',\n",
              " 'how are things\\tHOW THINGS',\n",
              " 'how can i help you\\tHOW I HELP YOU',\n",
              " 'how can i trust you\\tHOW I TRUST YOU',\n",
              " 'how dare you\\tHOW DARE YOU',\n",
              " 'how old are you\\tHOW OLD YOU',\n",
              " 'i am (age)\\tI (AGE)',\n",
              " 'i am afraid of that\\tI AFRAID THAT',\n",
              " 'i am crying\\tI CRY',\n",
              " 'i am feeling bored\\tI FEELING BORED',\n",
              " 'i am feeling cold\\tI FEELING COLD',\n",
              " 'i am fine. thank you sir\\tI FINE. THANK YOU SIR',\n",
              " 'i am hungry\\tI HUNGRY',\n",
              " 'i am in dilemma what to do\\tI IN DILEMMA WHAT TO DO',\n",
              " 'i am not really sure\\tI NOT REALLY SURE',\n",
              " 'i am really grateful\\tI REALLY GRATEFUL',\n",
              " 'i am sitting in the class\\tI SITTING IN THE CLASS',\n",
              " 'i am so sorry to hear that\\tI AM SO SORRY TO HEAR THAT',\n",
              " 'i am suffering from fever\\tI SUFFERING FROM FEVER',\n",
              " 'i am tired\\tI TIRED',\n",
              " 'i am very happy\\tI VERY HAPPY',\n",
              " 'i can not help you there\\tI NOT HELP YOU THERE',\n",
              " 'i do not agree\\tI DONOT AGREE',\n",
              " 'i do not like it\\tI DO NOT LIKE IT',\n",
              " 'i do not mean it\\tI DO NOT MEAN IT',\n",
              " 'i dont agree\\tI DONT AGREE',\n",
              " 'i enjoyed a lot\\tI ENJOYED A LOT',\n",
              " 'i got hurt\\tI GOT HURT',\n",
              " 'i like you i love you\\tI LIKE YOU I LOVE YOU',\n",
              " 'i need water\\tI NEED WATER',\n",
              " 'i promise\\tI PROMISE',\n",
              " 'i really appreciate it\\tI REALLY APPRECIATE IT',\n",
              " 'i somehow got to know about it\\tI SOMEHOW GOT KNOW ABOUT IT',\n",
              " 'i was stopped by some one\\tI STOPPED BY SOMEONE',\n",
              " 'it does not make any difference to me\\tIT DO NOT MAKE ANY DIFFERENCE TO ME',\n",
              " 'it was nice chatting with you\\tIT NICE CHAT WITH YOU',\n",
              " 'let him take time\\tLET HIM TAKE TIME',\n",
              " 'my name is xxxxxxxx\\tMY NAME XXXXXXXX',\n",
              " 'nice to meet you\\tNICE MEET YOU',\n",
              " 'No need to worry dont worry\\tNO NEED WORRY DONT WORRY',\n",
              " 'now onwards he will never hurt you\\tNOW ONWARDS HE NEVER HURT YOU',\n",
              " 'pour some more water into the glass\\tPOUR SOME MORE WATER INTO THE GLASS',\n",
              " 'prepare the bed\\tPREPARE BED',\n",
              " 'serve the food\\tSERVE FOOD',\n",
              " 'shall we go outside\\tWE GO OUTSIDE',\n",
              " 'speak softly\\tSPEAK SOFTLY',\n",
              " 'take care of yourself\\tTAKE CARE OF YOURSELF',\n",
              " 'tell me truth\\tTELL ME TRUTH',\n",
              " 'thank you so much\\tTHANK YOU SO MUCH',\n",
              " 'that is so kind of you\\tTHAT KIND YOU',\n",
              " 'This place is beautiful\\tTHIS PLACE BEAUTIFUL',\n",
              " 'try to understand\\tTRY TO UNDERSTAND',\n",
              " 'turn on light turn off light\\tTURN ON LIGHT TURN OFF LIGHT',\n",
              " 'we are all with you\\tWE ALL WITH YOU',\n",
              " 'wear the shirt\\tWEAR SHIRT',\n",
              " 'what are you doing\\tWHAT YOU DO',\n",
              " 'what did you tell him\\tWHAT YOU TELL HIM',\n",
              " 'what do you do\\tWHAT YOU DO',\n",
              " 'what do you think\\tWHAT DO YOU THINK',\n",
              " 'what do you want to become\\tWHAT DO YOU WANT BECOME',\n",
              " 'what happened\\tWHAT HAPPENED',\n",
              " 'what have you planned for your career\\tWHAT HAVE YOU PLAN YOUR CAREER',\n",
              " 'what is your phone number\\tWHAT YOUR PHONE NUMBER',\n",
              " 'what you want\\tWHAT YOU WANT',\n",
              " 'when will the train leave\\tWHEN TRAIN LEAVE',\n",
              " 'where are you from\\tWHERE YOU FROM',\n",
              " 'which college school are you from\\tWHICH COLLEGE SCHOOL YOU FROM',\n",
              " 'who are you\\tWHO YOU',\n",
              " 'why are you angry\\tWHY YOU ANGRY',\n",
              " 'why are you crying\\tWHY YOU CRY',\n",
              " 'why are you disappointed\\tWHY YOU DISAPPOINTED',\n",
              " 'you are bad\\tYOU BAD',\n",
              " 'you are good\\tYOU GOOD',\n",
              " 'you are welcome\\tYOU WELCOME',\n",
              " 'you can do it\\tYOU DO IT',\n",
              " 'you do anything, i do not care\\tYOU DO ANYTHING, I DO NOT CARE',\n",
              " 'you need a medicine, take this one\\tYOU NEED A MEDICINE, TAKE THIS ONE']"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "lines = open(txt_file_path, encoding='UTF-8').read().strip().split('\\n')\n",
        "lines"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pUocM7G0A3OI",
        "outputId": "e2eafe55-c7a5-4446-a8ca-2e812c6585f4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "total number of records:  101\n"
          ]
        }
      ],
      "source": [
        "print(\"total number of records: \",len(lines))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xr__GyO48f1c"
      },
      "source": [
        "##**Clean and Preprocess the text**\n",
        "\n",
        "1. Convert to lower case\n",
        "2. Convert special characters\n",
        "3. Remove Digits\n",
        "4. Remove spaces\n",
        "5. Add start and end tags to each sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "CFBP3ChaDfDs"
      },
      "outputs": [],
      "source": [
        "# Converts the unicode file to ascii\n",
        "def unicode_to_ascii(s):\n",
        "  return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
        "                 if unicodedata.category(c) != 'Mn')\n",
        "\n",
        "\n",
        "def preprocess_sentence(w):\n",
        "  w = unicode_to_ascii(w.lower().strip())\n",
        "\n",
        "  # creating a space between a word and the punctuation following it\n",
        "  # eg: \"he is a boy.\" => \"he is a boy .\"\n",
        "  # Reference:- https://stackoverflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation\n",
        "  w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
        "  w = re.sub(r'[\" \"]+', \" \", w)\n",
        "\n",
        "  # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
        "  w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n",
        "\n",
        "  w = w.strip()\n",
        "\n",
        "  # adding a start and an end token to the sentence\n",
        "  # so that the model know when to start and stop predicting.\n",
        "  w = '<start> ' + w + ' <end>'\n",
        "  print(w)\n",
        "  return w\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "XbvIRUMdpCEw"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ncK7mdB8dFb",
        "outputId": "b779080e-65ea-40f9-94e4-a5b712e0a5ae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<start> are you free today <end>\n",
            "<start> are you free today <end>\n",
            "<start> you free today <end>\n",
            "b'<start> you free today <end>'\n"
          ]
        }
      ],
      "source": [
        "en_sentence = u\"are you free today\"\n",
        "deu_sentence = u\"you free today\"\n",
        "print(preprocess_sentence(en_sentence))\n",
        "print(preprocess_sentence(deu_sentence).encode('utf-8'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U8C8W3HiF3OH"
      },
      "source": [
        "##**Generate pairs of cleaned English and sign  sentences with start and end added**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MjdPJpzq8c9z",
        "outputId": "5a9c82fb-ee4b-4bd6-b218-3e0ca83751cb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<start> are you free today <end>\n",
            "<start> you free today <end>\n",
            "<start> are you hiding something <end>\n",
            "<start> you hide something <end>\n",
            "<start> bring water for me <end>\n",
            "<start> bring water me <end>\n",
            "<start> can i help you <end>\n",
            "<start> i help you <end>\n",
            "<start> can you repeat that please <end>\n",
            "<start> you repeat please <end>\n",
            "<start> comb your hair <end>\n",
            "<start> comb you hair <end>\n",
            "<start> congratulations <end>\n",
            "<start> congratulatiions <end>\n",
            "<start> could you please talk slower <end>\n",
            "<start> you please talk slower <end>\n",
            "<start> do me a favour <end>\n",
            "<start> do me favour <end>\n",
            "<start> do not abuse him <end>\n",
            "<start> donot abuse him <end>\n",
            "<start> do not be stubborn <end>\n",
            "<start> donot be stubborn <end>\n",
            "<start> do not hurt me <end>\n",
            "<start> donot hurt me <end>\n",
            "<start> do not make me angry <end>\n",
            "<start> do not make me angry <end>\n",
            "<start> do not take it to the heart <end>\n",
            "<start> do not take it heart <end>\n",
            "<start> do not worry <end>\n",
            "<start> do not worry <end>\n",
            "<start> do you need something <end>\n",
            "<start> do you need something <end>\n",
            "<start> go and sleep <end>\n",
            "<start> go sleep <end>\n",
            "<start> had your food <end>\n",
            "<start> your food <end>\n",
            "<start> he came by train <end>\n",
            "<start> he came train <end>\n",
            "<start> he is going into the room <end>\n",
            "<start> he go into room <end>\n",
            "<start> he is on the way <end>\n",
            "<start> he on way <end>\n",
            "<start> he she is my friend <end>\n",
            "<start> he she my friend <end>\n",
            "<start> he would be coming today <end>\n",
            "<start> he coming today <end>\n",
            "<start> help me <end>\n",
            "<start> help me <end>\n",
            "<start> hi how are you <end>\n",
            "<start> hi how you <end>\n",
            "<start> how are things <end>\n",
            "<start> how things <end>\n",
            "<start> how can i help you <end>\n",
            "<start> how i help you <end>\n",
            "<start> how can i trust you <end>\n",
            "<start> how i trust you <end>\n",
            "<start> how dare you <end>\n",
            "<start> how dare you <end>\n",
            "<start> how old are you <end>\n",
            "<start> how old you <end>\n",
            "<start> i am age <end>\n",
            "<start> i age <end>\n",
            "<start> i am afraid of that <end>\n",
            "<start> i afraid that <end>\n",
            "<start> i am crying <end>\n",
            "<start> i cry <end>\n",
            "<start> i am feeling bored <end>\n",
            "<start> i feeling bored <end>\n",
            "<start> i am feeling cold <end>\n",
            "<start> i feeling cold <end>\n",
            "<start> i am fine . thank you sir <end>\n",
            "<start> i fine . thank you sir <end>\n",
            "<start> i am hungry <end>\n",
            "<start> i hungry <end>\n",
            "<start> i am in dilemma what to do <end>\n",
            "<start> i in dilemma what to do <end>\n",
            "<start> i am not really sure <end>\n",
            "<start> i not really sure <end>\n",
            "<start> i am really grateful <end>\n",
            "<start> i really grateful <end>\n",
            "<start> i am sitting in the class <end>\n",
            "<start> i sitting in the class <end>\n",
            "<start> i am so sorry to hear that <end>\n",
            "<start> i am so sorry to hear that <end>\n",
            "<start> i am suffering from fever <end>\n",
            "<start> i suffering from fever <end>\n",
            "<start> i am tired <end>\n",
            "<start> i tired <end>\n",
            "<start> i am very happy <end>\n",
            "<start> i very happy <end>\n",
            "<start> i can not help you there <end>\n",
            "<start> i not help you there <end>\n",
            "<start> i do not agree <end>\n",
            "<start> i donot agree <end>\n",
            "<start> i do not like it <end>\n",
            "<start> i do not like it <end>\n",
            "<start> i do not mean it <end>\n",
            "<start> i do not mean it <end>\n",
            "<start> i dont agree <end>\n",
            "<start> i dont agree <end>\n",
            "<start> i enjoyed a lot <end>\n",
            "<start> i enjoyed a lot <end>\n",
            "<start> i got hurt <end>\n",
            "<start> i got hurt <end>\n",
            "<start> i like you i love you <end>\n",
            "<start> i like you i love you <end>\n",
            "<start> i need water <end>\n",
            "<start> i need water <end>\n",
            "<start> i promise <end>\n",
            "<start> i promise <end>\n",
            "<start> i really appreciate it <end>\n",
            "<start> i really appreciate it <end>\n",
            "<start> i somehow got to know about it <end>\n",
            "<start> i somehow got know about it <end>\n",
            "<start> i was stopped by some one <end>\n",
            "<start> i stopped by someone <end>\n",
            "<start> it does not make any difference to me <end>\n",
            "<start> it do not make any difference to me <end>\n",
            "<start> it was nice chatting with you <end>\n",
            "<start> it nice chat with you <end>\n",
            "<start> let him take time <end>\n",
            "<start> let him take time <end>\n",
            "<start> my name is xxxxxxxx <end>\n",
            "<start> my name xxxxxxxx <end>\n",
            "<start> nice to meet you <end>\n",
            "<start> nice meet you <end>\n",
            "<start> no need to worry dont worry <end>\n",
            "<start> no need worry dont worry <end>\n",
            "<start> now onwards he will never hurt you <end>\n",
            "<start> now onwards he never hurt you <end>\n",
            "<start> pour some more water into the glass <end>\n",
            "<start> pour some more water into the glass <end>\n",
            "<start> prepare the bed <end>\n",
            "<start> prepare bed <end>\n",
            "<start> serve the food <end>\n",
            "<start> serve food <end>\n",
            "<start> shall we go outside <end>\n",
            "<start> we go outside <end>\n",
            "<start> speak softly <end>\n",
            "<start> speak softly <end>\n",
            "<start> take care of yourself <end>\n",
            "<start> take care of yourself <end>\n",
            "<start> tell me truth <end>\n",
            "<start> tell me truth <end>\n",
            "<start> thank you so much <end>\n",
            "<start> thank you so much <end>\n",
            "<start> that is so kind of you <end>\n",
            "<start> that kind you <end>\n",
            "<start> this place is beautiful <end>\n",
            "<start> this place beautiful <end>\n",
            "<start> try to understand <end>\n",
            "<start> try to understand <end>\n",
            "<start> turn on light turn off light <end>\n",
            "<start> turn on light turn off light <end>\n",
            "<start> we are all with you <end>\n",
            "<start> we all with you <end>\n",
            "<start> wear the shirt <end>\n",
            "<start> wear shirt <end>\n",
            "<start> what are you doing <end>\n",
            "<start> what you do <end>\n",
            "<start> what did you tell him <end>\n",
            "<start> what you tell him <end>\n",
            "<start> what do you do <end>\n",
            "<start> what you do <end>\n",
            "<start> what do you think <end>\n",
            "<start> what do you think <end>\n",
            "<start> what do you want to become <end>\n",
            "<start> what do you want become <end>\n",
            "<start> what happened <end>\n",
            "<start> what happened <end>\n",
            "<start> what have you planned for your career <end>\n",
            "<start> what have you plan your career <end>\n",
            "<start> what is your phone number <end>\n",
            "<start> what your phone number <end>\n",
            "<start> what you want <end>\n",
            "<start> what you want <end>\n",
            "<start> when will the train leave <end>\n",
            "<start> when train leave <end>\n",
            "<start> where are you from <end>\n",
            "<start> where you from <end>\n",
            "<start> which college school are you from <end>\n",
            "<start> which college school you from <end>\n",
            "<start> who are you <end>\n",
            "<start> who you <end>\n",
            "<start> why are you angry <end>\n",
            "<start> why you angry <end>\n",
            "<start> why are you crying <end>\n",
            "<start> why you cry <end>\n",
            "<start> why are you disappointed <end>\n",
            "<start> why you disappointed <end>\n",
            "<start> you are bad <end>\n",
            "<start> you bad <end>\n",
            "<start> you are good <end>\n",
            "<start> you good <end>\n",
            "<start> you are welcome <end>\n",
            "<start> you welcome <end>\n",
            "<start> you can do it <end>\n",
            "<start> you do it <end>\n",
            "<start> you do anything , i do not care <end>\n",
            "<start> you do anything , i do not care <end>\n",
            "<start> you need a medicine , take this one <end>\n",
            "<start> you need a medicine , take this one <end>\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[['<start> are you free today <end>', '<start> you free today <end>'],\n",
              " ['<start> are you hiding something <end>',\n",
              "  '<start> you hide something <end>'],\n",
              " ['<start> bring water for me <end>', '<start> bring water me <end>'],\n",
              " ['<start> can i help you <end>', '<start> i help you <end>'],\n",
              " ['<start> can you repeat that please <end>',\n",
              "  '<start> you repeat please <end>'],\n",
              " ['<start> comb your hair <end>', '<start> comb you hair <end>'],\n",
              " ['<start> congratulations <end>', '<start> congratulatiions <end>'],\n",
              " ['<start> could you please talk slower <end>',\n",
              "  '<start> you please talk slower <end>'],\n",
              " ['<start> do me a favour <end>', '<start> do me favour <end>'],\n",
              " ['<start> do not abuse him <end>', '<start> donot abuse him <end>'],\n",
              " ['<start> do not be stubborn <end>', '<start> donot be stubborn <end>'],\n",
              " ['<start> do not hurt me <end>', '<start> donot hurt me <end>'],\n",
              " ['<start> do not make me angry <end>', '<start> do not make me angry <end>'],\n",
              " ['<start> do not take it to the heart <end>',\n",
              "  '<start> do not take it heart <end>'],\n",
              " ['<start> do not worry <end>', '<start> do not worry <end>'],\n",
              " ['<start> do you need something <end>',\n",
              "  '<start> do you need something <end>'],\n",
              " ['<start> go and sleep <end>', '<start> go sleep <end>'],\n",
              " ['<start> had your food <end>', '<start> your food <end>'],\n",
              " ['<start> he came by train <end>', '<start> he came train <end>'],\n",
              " ['<start> he is going into the room <end>', '<start> he go into room <end>'],\n",
              " ['<start> he is on the way <end>', '<start> he on way <end>'],\n",
              " ['<start> he she is my friend <end>', '<start> he she my friend <end>'],\n",
              " ['<start> he would be coming today <end>', '<start> he coming today <end>'],\n",
              " ['<start> help me <end>', '<start> help me <end>'],\n",
              " ['<start> hi how are you <end>', '<start> hi how you <end>'],\n",
              " ['<start> how are things <end>', '<start> how things <end>'],\n",
              " ['<start> how can i help you <end>', '<start> how i help you <end>'],\n",
              " ['<start> how can i trust you <end>', '<start> how i trust you <end>'],\n",
              " ['<start> how dare you <end>', '<start> how dare you <end>'],\n",
              " ['<start> how old are you <end>', '<start> how old you <end>'],\n",
              " ['<start> i am age <end>', '<start> i age <end>'],\n",
              " ['<start> i am afraid of that <end>', '<start> i afraid that <end>'],\n",
              " ['<start> i am crying <end>', '<start> i cry <end>'],\n",
              " ['<start> i am feeling bored <end>', '<start> i feeling bored <end>'],\n",
              " ['<start> i am feeling cold <end>', '<start> i feeling cold <end>'],\n",
              " ['<start> i am fine . thank you sir <end>',\n",
              "  '<start> i fine . thank you sir <end>'],\n",
              " ['<start> i am hungry <end>', '<start> i hungry <end>'],\n",
              " ['<start> i am in dilemma what to do <end>',\n",
              "  '<start> i in dilemma what to do <end>'],\n",
              " ['<start> i am not really sure <end>', '<start> i not really sure <end>'],\n",
              " ['<start> i am really grateful <end>', '<start> i really grateful <end>'],\n",
              " ['<start> i am sitting in the class <end>',\n",
              "  '<start> i sitting in the class <end>'],\n",
              " ['<start> i am so sorry to hear that <end>',\n",
              "  '<start> i am so sorry to hear that <end>'],\n",
              " ['<start> i am suffering from fever <end>',\n",
              "  '<start> i suffering from fever <end>'],\n",
              " ['<start> i am tired <end>', '<start> i tired <end>'],\n",
              " ['<start> i am very happy <end>', '<start> i very happy <end>'],\n",
              " ['<start> i can not help you there <end>',\n",
              "  '<start> i not help you there <end>'],\n",
              " ['<start> i do not agree <end>', '<start> i donot agree <end>'],\n",
              " ['<start> i do not like it <end>', '<start> i do not like it <end>'],\n",
              " ['<start> i do not mean it <end>', '<start> i do not mean it <end>'],\n",
              " ['<start> i dont agree <end>', '<start> i dont agree <end>'],\n",
              " ['<start> i enjoyed a lot <end>', '<start> i enjoyed a lot <end>'],\n",
              " ['<start> i got hurt <end>', '<start> i got hurt <end>'],\n",
              " ['<start> i like you i love you <end>',\n",
              "  '<start> i like you i love you <end>'],\n",
              " ['<start> i need water <end>', '<start> i need water <end>'],\n",
              " ['<start> i promise <end>', '<start> i promise <end>'],\n",
              " ['<start> i really appreciate it <end>',\n",
              "  '<start> i really appreciate it <end>'],\n",
              " ['<start> i somehow got to know about it <end>',\n",
              "  '<start> i somehow got know about it <end>'],\n",
              " ['<start> i was stopped by some one <end>',\n",
              "  '<start> i stopped by someone <end>'],\n",
              " ['<start> it does not make any difference to me <end>',\n",
              "  '<start> it do not make any difference to me <end>'],\n",
              " ['<start> it was nice chatting with you <end>',\n",
              "  '<start> it nice chat with you <end>'],\n",
              " ['<start> let him take time <end>', '<start> let him take time <end>'],\n",
              " ['<start> my name is xxxxxxxx <end>', '<start> my name xxxxxxxx <end>'],\n",
              " ['<start> nice to meet you <end>', '<start> nice meet you <end>'],\n",
              " ['<start> no need to worry dont worry <end>',\n",
              "  '<start> no need worry dont worry <end>'],\n",
              " ['<start> now onwards he will never hurt you <end>',\n",
              "  '<start> now onwards he never hurt you <end>'],\n",
              " ['<start> pour some more water into the glass <end>',\n",
              "  '<start> pour some more water into the glass <end>'],\n",
              " ['<start> prepare the bed <end>', '<start> prepare bed <end>'],\n",
              " ['<start> serve the food <end>', '<start> serve food <end>'],\n",
              " ['<start> shall we go outside <end>', '<start> we go outside <end>'],\n",
              " ['<start> speak softly <end>', '<start> speak softly <end>'],\n",
              " ['<start> take care of yourself <end>',\n",
              "  '<start> take care of yourself <end>'],\n",
              " ['<start> tell me truth <end>', '<start> tell me truth <end>'],\n",
              " ['<start> thank you so much <end>', '<start> thank you so much <end>'],\n",
              " ['<start> that is so kind of you <end>', '<start> that kind you <end>'],\n",
              " ['<start> this place is beautiful <end>',\n",
              "  '<start> this place beautiful <end>'],\n",
              " ['<start> try to understand <end>', '<start> try to understand <end>'],\n",
              " ['<start> turn on light turn off light <end>',\n",
              "  '<start> turn on light turn off light <end>'],\n",
              " ['<start> we are all with you <end>', '<start> we all with you <end>'],\n",
              " ['<start> wear the shirt <end>', '<start> wear shirt <end>'],\n",
              " ['<start> what are you doing <end>', '<start> what you do <end>'],\n",
              " ['<start> what did you tell him <end>', '<start> what you tell him <end>'],\n",
              " ['<start> what do you do <end>', '<start> what you do <end>'],\n",
              " ['<start> what do you think <end>', '<start> what do you think <end>'],\n",
              " ['<start> what do you want to become <end>',\n",
              "  '<start> what do you want become <end>'],\n",
              " ['<start> what happened <end>', '<start> what happened <end>'],\n",
              " ['<start> what have you planned for your career <end>',\n",
              "  '<start> what have you plan your career <end>'],\n",
              " ['<start> what is your phone number <end>',\n",
              "  '<start> what your phone number <end>'],\n",
              " ['<start> what you want <end>', '<start> what you want <end>'],\n",
              " ['<start> when will the train leave <end>', '<start> when train leave <end>'],\n",
              " ['<start> where are you from <end>', '<start> where you from <end>'],\n",
              " ['<start> which college school are you from <end>',\n",
              "  '<start> which college school you from <end>'],\n",
              " ['<start> who are you <end>', '<start> who you <end>'],\n",
              " ['<start> why are you angry <end>', '<start> why you angry <end>'],\n",
              " ['<start> why are you crying <end>', '<start> why you cry <end>'],\n",
              " ['<start> why are you disappointed <end>',\n",
              "  '<start> why you disappointed <end>'],\n",
              " ['<start> you are bad <end>', '<start> you bad <end>'],\n",
              " ['<start> you are good <end>', '<start> you good <end>'],\n",
              " ['<start> you are welcome <end>', '<start> you welcome <end>'],\n",
              " ['<start> you can do it <end>', '<start> you do it <end>'],\n",
              " ['<start> you do anything , i do not care <end>',\n",
              "  '<start> you do anything , i do not care <end>'],\n",
              " ['<start> you need a medicine , take this one <end>',\n",
              "  '<start> you need a medicine , take this one <end>']]"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Generate pairs of cleaned English and Deutch sentences\n",
        "sent_pairs = []\n",
        "#due to memory constraints we'd not be using the whole data (227080 sentences)\n",
        "for line in lines[:102]:\n",
        "    sent_pair = []\n",
        "    sentence = line.rstrip().split('\\t')[0]\n",
        "    target = line.rstrip().split('\\t')[1]\n",
        "    sentence = preprocess_sentence(sentence)\n",
        "    sent_pair.append(sentence)\n",
        "    target = preprocess_sentence(target)\n",
        "    sent_pair.append(target)\n",
        "    sent_pairs.append(sent_pair)\n",
        "sent_pairs[:102]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nlGunOWPeOCz",
        "outputId": "4c0f90cf-bc53-4fc8-cd7e-1b33d80fd2cc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['are you free today\\tYOU FREE TODAY',\n",
              " 'are you hiding something\\tYOU HIDE SOMETHING',\n",
              " 'bring water for me\\tBRING WATER ME',\n",
              " 'can i help you\\tI HELP YOU',\n",
              " 'can you repeat that please\\tYOU REPEAT PLEASE',\n",
              " 'comb your hair\\tCOMB YOU HAIR',\n",
              " 'congratulations\\tCONGRATULATIIONS',\n",
              " 'could you please talk slower\\tYOU PLEASE TALK SLOWER',\n",
              " 'do me a favour\\tDO ME FAVOUR',\n",
              " 'do not abuse him\\tDONOT ABUSE HIM',\n",
              " 'do not be stubborn\\tDONOT BE STUBBORN',\n",
              " 'do not hurt me \\tDONOT HURT ME',\n",
              " 'do not make me angry\\tDO NOT MAKE ME ANGRY',\n",
              " 'do not take it to the heart\\tDO NOT TAKE IT HEART',\n",
              " 'do not worry\\tDO NOT WORRY',\n",
              " 'do you need something\\tDO YOU NEED SOMETHING',\n",
              " 'go and sleep\\tGO SLEEP',\n",
              " 'had your food\\tYOUR FOOD',\n",
              " 'he came by train\\tHE CAME TRAIN',\n",
              " 'He is going into the room\\tHE GO  INTO  ROOM',\n",
              " 'he is on the way\\tHE ON WAY',\n",
              " 'he she is my friend\\tHE SHE MY FRIEND',\n",
              " 'he would be coming today\\tHE COMING TODAY',\n",
              " 'help me\\tHELP ME',\n",
              " 'hi how are you\\tHI HOW YOU',\n",
              " 'how are things\\tHOW THINGS',\n",
              " 'how can i help you\\tHOW I HELP YOU',\n",
              " 'how can i trust you\\tHOW I TRUST YOU',\n",
              " 'how dare you\\tHOW DARE YOU',\n",
              " 'how old are you\\tHOW OLD YOU',\n",
              " 'i am (age)\\tI (AGE)',\n",
              " 'i am afraid of that\\tI AFRAID THAT',\n",
              " 'i am crying\\tI CRY',\n",
              " 'i am feeling bored\\tI FEELING BORED',\n",
              " 'i am feeling cold\\tI FEELING COLD',\n",
              " 'i am fine. thank you sir\\tI FINE. THANK YOU SIR',\n",
              " 'i am hungry\\tI HUNGRY',\n",
              " 'i am in dilemma what to do\\tI IN DILEMMA WHAT TO DO',\n",
              " 'i am not really sure\\tI NOT REALLY SURE',\n",
              " 'i am really grateful\\tI REALLY GRATEFUL',\n",
              " 'i am sitting in the class\\tI SITTING IN THE CLASS',\n",
              " 'i am so sorry to hear that\\tI AM SO SORRY TO HEAR THAT',\n",
              " 'i am suffering from fever\\tI SUFFERING FROM FEVER',\n",
              " 'i am tired\\tI TIRED',\n",
              " 'i am very happy\\tI VERY HAPPY',\n",
              " 'i can not help you there\\tI NOT HELP YOU THERE',\n",
              " 'i do not agree\\tI DONOT AGREE',\n",
              " 'i do not like it\\tI DO NOT LIKE IT',\n",
              " 'i do not mean it\\tI DO NOT MEAN IT',\n",
              " 'i dont agree\\tI DONT AGREE',\n",
              " 'i enjoyed a lot\\tI ENJOYED A LOT',\n",
              " 'i got hurt\\tI GOT HURT',\n",
              " 'i like you i love you\\tI LIKE YOU I LOVE YOU',\n",
              " 'i need water\\tI NEED WATER',\n",
              " 'i promise\\tI PROMISE',\n",
              " 'i really appreciate it\\tI REALLY APPRECIATE IT',\n",
              " 'i somehow got to know about it\\tI SOMEHOW GOT KNOW ABOUT IT',\n",
              " 'i was stopped by some one\\tI STOPPED BY SOMEONE',\n",
              " 'it does not make any difference to me\\tIT DO NOT MAKE ANY DIFFERENCE TO ME',\n",
              " 'it was nice chatting with you\\tIT NICE CHAT WITH YOU',\n",
              " 'let him take time\\tLET HIM TAKE TIME',\n",
              " 'my name is xxxxxxxx\\tMY NAME XXXXXXXX',\n",
              " 'nice to meet you\\tNICE MEET YOU',\n",
              " 'No need to worry dont worry\\tNO NEED WORRY DONT WORRY',\n",
              " 'now onwards he will never hurt you\\tNOW ONWARDS HE NEVER HURT YOU',\n",
              " 'pour some more water into the glass\\tPOUR SOME MORE WATER INTO THE GLASS',\n",
              " 'prepare the bed\\tPREPARE BED',\n",
              " 'serve the food\\tSERVE FOOD',\n",
              " 'shall we go outside\\tWE GO OUTSIDE',\n",
              " 'speak softly\\tSPEAK SOFTLY',\n",
              " 'take care of yourself\\tTAKE CARE OF YOURSELF',\n",
              " 'tell me truth\\tTELL ME TRUTH',\n",
              " 'thank you so much\\tTHANK YOU SO MUCH',\n",
              " 'that is so kind of you\\tTHAT KIND YOU',\n",
              " 'This place is beautiful\\tTHIS PLACE BEAUTIFUL',\n",
              " 'try to understand\\tTRY TO UNDERSTAND',\n",
              " 'turn on light turn off light\\tTURN ON LIGHT TURN OFF LIGHT',\n",
              " 'we are all with you\\tWE ALL WITH YOU',\n",
              " 'wear the shirt\\tWEAR SHIRT',\n",
              " 'what are you doing\\tWHAT YOU DO',\n",
              " 'what did you tell him\\tWHAT YOU TELL HIM',\n",
              " 'what do you do\\tWHAT YOU DO',\n",
              " 'what do you think\\tWHAT DO YOU THINK',\n",
              " 'what do you want to become\\tWHAT DO YOU WANT BECOME',\n",
              " 'what happened\\tWHAT HAPPENED',\n",
              " 'what have you planned for your career\\tWHAT HAVE YOU PLAN YOUR CAREER',\n",
              " 'what is your phone number\\tWHAT YOUR PHONE NUMBER',\n",
              " 'what you want\\tWHAT YOU WANT',\n",
              " 'when will the train leave\\tWHEN TRAIN LEAVE',\n",
              " 'where are you from\\tWHERE YOU FROM',\n",
              " 'which college school are you from\\tWHICH COLLEGE SCHOOL YOU FROM',\n",
              " 'who are you\\tWHO YOU',\n",
              " 'why are you angry\\tWHY YOU ANGRY',\n",
              " 'why are you crying\\tWHY YOU CRY',\n",
              " 'why are you disappointed\\tWHY YOU DISAPPOINTED',\n",
              " 'you are bad\\tYOU BAD',\n",
              " 'you are good\\tYOU GOOD',\n",
              " 'you are welcome\\tYOU WELCOME',\n",
              " 'you can do it\\tYOU DO IT',\n",
              " 'you do anything, i do not care\\tYOU DO ANYTHING, I DO NOT CARE',\n",
              " 'you need a medicine, take this one\\tYOU NEED A MEDICINE, TAKE THIS ONE']"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "lines"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8cy43B2HHLkF"
      },
      "source": [
        "##**Create a class to map every word to an index and vice-versa for any given vocabulary.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "L3n2TkKV8c7e"
      },
      "outputs": [],
      "source": [
        "# This class creates a word -> index mapping (e.g,. \"dad\" -> 5) and vice-versa\n",
        "# (e.g., 5 -> \"dad\") for each language,\n",
        "class LanguageIndex():\n",
        "    def __init__(self, lang):\n",
        "        self.lang = lang\n",
        "        self.word2idx = {}\n",
        "        self.idx2word = {}\n",
        "        self.vocab = set()\n",
        "\n",
        "        self.create_index()\n",
        "\n",
        "    def create_index(self):\n",
        "        for phrase in self.lang:\n",
        "            self.vocab.update(phrase.split(' '))\n",
        "\n",
        "        self.vocab = sorted(self.vocab)\n",
        "\n",
        "        self.word2idx['<pad>'] = 0\n",
        "        for index, word in enumerate(self.vocab):\n",
        "            self.word2idx[word] = index + 1\n",
        "\n",
        "        for word, index in self.word2idx.items():\n",
        "            self.idx2word[index] = word"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "xQcluJ3m8c4_"
      },
      "outputs": [],
      "source": [
        "def max_length(tensor):\n",
        "    return max(len(t) for t in tensor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i09jLuGCeyoo",
        "outputId": "bbc31d6b-75e8-43a0-feea-52ccd7edcf1d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[['<start> are you free today <end>', '<start> you free today <end>'],\n",
              " ['<start> are you hiding something <end>',\n",
              "  '<start> you hide something <end>'],\n",
              " ['<start> bring water for me <end>', '<start> bring water me <end>'],\n",
              " ['<start> can i help you <end>', '<start> i help you <end>'],\n",
              " ['<start> can you repeat that please <end>',\n",
              "  '<start> you repeat please <end>'],\n",
              " ['<start> comb your hair <end>', '<start> comb you hair <end>'],\n",
              " ['<start> congratulations <end>', '<start> congratulatiions <end>'],\n",
              " ['<start> could you please talk slower <end>',\n",
              "  '<start> you please talk slower <end>'],\n",
              " ['<start> do me a favour <end>', '<start> do me favour <end>'],\n",
              " ['<start> do not abuse him <end>', '<start> donot abuse him <end>'],\n",
              " ['<start> do not be stubborn <end>', '<start> donot be stubborn <end>'],\n",
              " ['<start> do not hurt me <end>', '<start> donot hurt me <end>'],\n",
              " ['<start> do not make me angry <end>', '<start> do not make me angry <end>'],\n",
              " ['<start> do not take it to the heart <end>',\n",
              "  '<start> do not take it heart <end>'],\n",
              " ['<start> do not worry <end>', '<start> do not worry <end>'],\n",
              " ['<start> do you need something <end>',\n",
              "  '<start> do you need something <end>'],\n",
              " ['<start> go and sleep <end>', '<start> go sleep <end>'],\n",
              " ['<start> had your food <end>', '<start> your food <end>'],\n",
              " ['<start> he came by train <end>', '<start> he came train <end>'],\n",
              " ['<start> he is going into the room <end>', '<start> he go into room <end>'],\n",
              " ['<start> he is on the way <end>', '<start> he on way <end>'],\n",
              " ['<start> he she is my friend <end>', '<start> he she my friend <end>'],\n",
              " ['<start> he would be coming today <end>', '<start> he coming today <end>'],\n",
              " ['<start> help me <end>', '<start> help me <end>'],\n",
              " ['<start> hi how are you <end>', '<start> hi how you <end>'],\n",
              " ['<start> how are things <end>', '<start> how things <end>'],\n",
              " ['<start> how can i help you <end>', '<start> how i help you <end>'],\n",
              " ['<start> how can i trust you <end>', '<start> how i trust you <end>'],\n",
              " ['<start> how dare you <end>', '<start> how dare you <end>'],\n",
              " ['<start> how old are you <end>', '<start> how old you <end>'],\n",
              " ['<start> i am age <end>', '<start> i age <end>'],\n",
              " ['<start> i am afraid of that <end>', '<start> i afraid that <end>'],\n",
              " ['<start> i am crying <end>', '<start> i cry <end>'],\n",
              " ['<start> i am feeling bored <end>', '<start> i feeling bored <end>'],\n",
              " ['<start> i am feeling cold <end>', '<start> i feeling cold <end>'],\n",
              " ['<start> i am fine . thank you sir <end>',\n",
              "  '<start> i fine . thank you sir <end>'],\n",
              " ['<start> i am hungry <end>', '<start> i hungry <end>'],\n",
              " ['<start> i am in dilemma what to do <end>',\n",
              "  '<start> i in dilemma what to do <end>'],\n",
              " ['<start> i am not really sure <end>', '<start> i not really sure <end>'],\n",
              " ['<start> i am really grateful <end>', '<start> i really grateful <end>'],\n",
              " ['<start> i am sitting in the class <end>',\n",
              "  '<start> i sitting in the class <end>'],\n",
              " ['<start> i am so sorry to hear that <end>',\n",
              "  '<start> i am so sorry to hear that <end>'],\n",
              " ['<start> i am suffering from fever <end>',\n",
              "  '<start> i suffering from fever <end>'],\n",
              " ['<start> i am tired <end>', '<start> i tired <end>'],\n",
              " ['<start> i am very happy <end>', '<start> i very happy <end>'],\n",
              " ['<start> i can not help you there <end>',\n",
              "  '<start> i not help you there <end>'],\n",
              " ['<start> i do not agree <end>', '<start> i donot agree <end>'],\n",
              " ['<start> i do not like it <end>', '<start> i do not like it <end>'],\n",
              " ['<start> i do not mean it <end>', '<start> i do not mean it <end>'],\n",
              " ['<start> i dont agree <end>', '<start> i dont agree <end>'],\n",
              " ['<start> i enjoyed a lot <end>', '<start> i enjoyed a lot <end>'],\n",
              " ['<start> i got hurt <end>', '<start> i got hurt <end>'],\n",
              " ['<start> i like you i love you <end>',\n",
              "  '<start> i like you i love you <end>'],\n",
              " ['<start> i need water <end>', '<start> i need water <end>'],\n",
              " ['<start> i promise <end>', '<start> i promise <end>'],\n",
              " ['<start> i really appreciate it <end>',\n",
              "  '<start> i really appreciate it <end>'],\n",
              " ['<start> i somehow got to know about it <end>',\n",
              "  '<start> i somehow got know about it <end>'],\n",
              " ['<start> i was stopped by some one <end>',\n",
              "  '<start> i stopped by someone <end>'],\n",
              " ['<start> it does not make any difference to me <end>',\n",
              "  '<start> it do not make any difference to me <end>'],\n",
              " ['<start> it was nice chatting with you <end>',\n",
              "  '<start> it nice chat with you <end>'],\n",
              " ['<start> let him take time <end>', '<start> let him take time <end>'],\n",
              " ['<start> my name is xxxxxxxx <end>', '<start> my name xxxxxxxx <end>'],\n",
              " ['<start> nice to meet you <end>', '<start> nice meet you <end>'],\n",
              " ['<start> no need to worry dont worry <end>',\n",
              "  '<start> no need worry dont worry <end>'],\n",
              " ['<start> now onwards he will never hurt you <end>',\n",
              "  '<start> now onwards he never hurt you <end>'],\n",
              " ['<start> pour some more water into the glass <end>',\n",
              "  '<start> pour some more water into the glass <end>'],\n",
              " ['<start> prepare the bed <end>', '<start> prepare bed <end>'],\n",
              " ['<start> serve the food <end>', '<start> serve food <end>'],\n",
              " ['<start> shall we go outside <end>', '<start> we go outside <end>'],\n",
              " ['<start> speak softly <end>', '<start> speak softly <end>'],\n",
              " ['<start> take care of yourself <end>',\n",
              "  '<start> take care of yourself <end>'],\n",
              " ['<start> tell me truth <end>', '<start> tell me truth <end>'],\n",
              " ['<start> thank you so much <end>', '<start> thank you so much <end>'],\n",
              " ['<start> that is so kind of you <end>', '<start> that kind you <end>'],\n",
              " ['<start> this place is beautiful <end>',\n",
              "  '<start> this place beautiful <end>'],\n",
              " ['<start> try to understand <end>', '<start> try to understand <end>'],\n",
              " ['<start> turn on light turn off light <end>',\n",
              "  '<start> turn on light turn off light <end>'],\n",
              " ['<start> we are all with you <end>', '<start> we all with you <end>'],\n",
              " ['<start> wear the shirt <end>', '<start> wear shirt <end>'],\n",
              " ['<start> what are you doing <end>', '<start> what you do <end>'],\n",
              " ['<start> what did you tell him <end>', '<start> what you tell him <end>'],\n",
              " ['<start> what do you do <end>', '<start> what you do <end>'],\n",
              " ['<start> what do you think <end>', '<start> what do you think <end>'],\n",
              " ['<start> what do you want to become <end>',\n",
              "  '<start> what do you want become <end>'],\n",
              " ['<start> what happened <end>', '<start> what happened <end>'],\n",
              " ['<start> what have you planned for your career <end>',\n",
              "  '<start> what have you plan your career <end>'],\n",
              " ['<start> what is your phone number <end>',\n",
              "  '<start> what your phone number <end>'],\n",
              " ['<start> what you want <end>', '<start> what you want <end>'],\n",
              " ['<start> when will the train leave <end>', '<start> when train leave <end>'],\n",
              " ['<start> where are you from <end>', '<start> where you from <end>'],\n",
              " ['<start> which college school are you from <end>',\n",
              "  '<start> which college school you from <end>'],\n",
              " ['<start> who are you <end>', '<start> who you <end>'],\n",
              " ['<start> why are you angry <end>', '<start> why you angry <end>'],\n",
              " ['<start> why are you crying <end>', '<start> why you cry <end>'],\n",
              " ['<start> why are you disappointed <end>',\n",
              "  '<start> why you disappointed <end>'],\n",
              " ['<start> you are bad <end>', '<start> you bad <end>'],\n",
              " ['<start> you are good <end>', '<start> you good <end>'],\n",
              " ['<start> you are welcome <end>', '<start> you welcome <end>'],\n",
              " ['<start> you can do it <end>', '<start> you do it <end>'],\n",
              " ['<start> you do anything , i do not care <end>',\n",
              "  '<start> you do anything , i do not care <end>'],\n",
              " ['<start> you need a medicine , take this one <end>',\n",
              "  '<start> you need a medicine , take this one <end>']]"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sent_pairs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NNvdQq5qICjF"
      },
      "source": [
        "##**Tokenization and Padding**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "nzjMZc598c2f"
      },
      "outputs": [],
      "source": [
        "def load_dataset(pairs, num_examples):\n",
        "    # pairs => already created cleaned input, output pairs\n",
        "\n",
        "    # index language using the class defined above\n",
        "    # Combine input and target sentences into a single list\n",
        "    all_sentences = [en for en, de in pairs] + [de for en, de in pairs]\n",
        "\n",
        "    # Create a single LanguageIndex instance for both languages\n",
        "    lang = LanguageIndex(all_sentences)\n",
        "\n",
        "    # Vectorize the input and target languages\n",
        "    # spoken sentence and sign sentence\n",
        "    input_tensor = [[lang.word2idx[s] for s in en.split(' ')] for en, de in pairs]\n",
        "    target_tensor = [[lang.word2idx[s] for s in de.split(' ')] for en, de in pairs]\n",
        "    print(input_tensor)\n",
        "    print(target_tensor)\n",
        "    # Calculate max_length of input and output tensor\n",
        "    # Here, we'll set those to the longest sentence in the dataset\n",
        "    max_length_inp, max_length_tar = max_length(input_tensor), max_length(target_tensor)\n",
        "\n",
        "    # Padding the input and output tensor to the maximum length\n",
        "    input_tensor = tf.keras.preprocessing.sequence.pad_sequences(input_tensor,\n",
        "                                                                maxlen=max_length_inp,\n",
        "                                                                padding='post')\n",
        "    target_tensor = tf.keras.preprocessing.sequence.pad_sequences(target_tensor,\n",
        "                                                                  maxlen=max_length_tar,\n",
        "                                                                  padding='post')\n",
        "\n",
        "    return input_tensor, target_tensor, lang, max_length_inp, max_length_tar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2hzdpLRE8c0L",
        "outputId": "9decde56-cd99-4572-9574-b75f54531af1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[4, 18, 192, 60, 166, 3], [4, 18, 192, 80, 146, 3], [4, 25, 176, 59, 99, 3], [4, 28, 85, 77, 192, 3], [4, 28, 192, 130, 157, 125, 3], [4, 36, 193, 70, 3], [4, 39, 3], [4, 40, 192, 125, 154, 140, 3], [4, 48, 99, 5, 54, 3], [4, 48, 111, 7, 81, 3], [4, 48, 111, 20, 150, 3], [4, 48, 111, 84, 99, 3], [4, 48, 111, 98, 99, 14, 3], [4, 48, 111, 153, 89, 165, 158, 76, 3], [4, 48, 111, 189, 3], [4, 48, 192, 107, 146, 3], [4, 64, 13, 139, 3], [4, 69, 193, 58, 3], [4, 74, 27, 26, 167, 3], [4, 74, 88, 65, 87, 158, 131, 3], [4, 74, 88, 117, 158, 177, 3], [4, 74, 135, 88, 105, 61, 3], [4, 74, 190, 20, 37, 166, 3], [4, 77, 99, 3], [4, 78, 82, 18, 192, 3], [4, 82, 18, 160, 3], [4, 82, 28, 85, 77, 192, 3], [4, 82, 28, 85, 168, 192, 3], [4, 82, 43, 192, 3], [4, 82, 116, 18, 192, 3], [4, 85, 12, 9, 3], [4, 85, 12, 8, 114, 157, 3], [4, 85, 12, 42, 3], [4, 85, 12, 55, 24, 3], [4, 85, 12, 55, 34, 3], [4, 85, 12, 57, 2, 156, 192, 137, 3], [4, 85, 12, 83, 3], [4, 85, 12, 86, 46, 181, 165, 48, 3], [4, 85, 12, 111, 129, 152, 3], [4, 85, 12, 129, 68, 3], [4, 85, 12, 138, 86, 158, 33, 3], [4, 85, 12, 141, 147, 165, 75, 157, 3], [4, 85, 12, 151, 62, 56, 3], [4, 85, 12, 164, 3], [4, 85, 12, 173, 72, 3], [4, 85, 28, 111, 77, 192, 159, 3], [4, 85, 48, 111, 10, 3], [4, 85, 48, 111, 95, 89, 3], [4, 85, 48, 111, 100, 89, 3], [4, 85, 52, 10, 3], [4, 85, 53, 5, 96, 3], [4, 85, 67, 84, 3], [4, 85, 95, 192, 85, 97, 192, 3], [4, 85, 107, 176, 3], [4, 85, 128, 3], [4, 85, 129, 17, 89, 3], [4, 85, 144, 67, 165, 91, 6, 89, 3], [4, 85, 175, 149, 26, 143, 118, 3], [4, 89, 49, 111, 98, 15, 45, 165, 99, 3], [4, 89, 175, 109, 32, 188, 192, 3], [4, 93, 81, 153, 163, 3], [4, 105, 106, 88, 191, 3], [4, 109, 165, 102, 192, 3], [4, 110, 107, 165, 189, 52, 189, 3], [4, 112, 119, 74, 187, 108, 84, 192, 3], [4, 126, 143, 103, 176, 87, 158, 63, 3], [4, 127, 158, 23, 3], [4, 133, 158, 58, 3], [4, 134, 178, 64, 120, 3], [4, 148, 142, 3], [4, 153, 29, 114, 194, 3], [4, 155, 99, 169, 3], [4, 156, 192, 141, 104, 3], [4, 157, 88, 141, 90, 114, 192, 3], [4, 162, 122, 88, 21, 3], [4, 170, 165, 172, 3], [4, 171, 117, 94, 171, 115, 94, 3], [4, 178, 18, 11, 188, 192, 3], [4, 179, 158, 136, 3], [4, 181, 18, 192, 50, 3], [4, 181, 44, 192, 155, 81, 3], [4, 181, 48, 192, 48, 3], [4, 181, 48, 192, 161, 3], [4, 181, 48, 192, 174, 165, 22, 3], [4, 181, 71, 3], [4, 181, 73, 192, 124, 59, 193, 30, 3], [4, 181, 88, 193, 121, 113, 3], [4, 181, 192, 174, 3], [4, 182, 187, 158, 167, 92, 3], [4, 183, 18, 192, 62, 3], [4, 184, 35, 132, 18, 192, 62, 3], [4, 185, 18, 192, 3], [4, 186, 18, 192, 14, 3], [4, 186, 18, 192, 42, 3], [4, 186, 18, 192, 47, 3], [4, 192, 18, 19, 3], [4, 192, 18, 66, 3], [4, 192, 18, 180, 3], [4, 192, 28, 48, 89, 3], [4, 192, 48, 16, 1, 85, 48, 111, 29, 3], [4, 192, 107, 5, 101, 1, 153, 162, 118, 3]]\n",
            "[[4, 192, 60, 166, 3], [4, 192, 79, 146, 3], [4, 25, 176, 99, 3], [4, 85, 77, 192, 3], [4, 192, 130, 125, 3], [4, 36, 192, 70, 3], [4, 38, 3], [4, 192, 125, 154, 140, 3], [4, 48, 99, 54, 3], [4, 51, 7, 81, 3], [4, 51, 20, 150, 3], [4, 51, 84, 99, 3], [4, 48, 111, 98, 99, 14, 3], [4, 48, 111, 153, 89, 76, 3], [4, 48, 111, 189, 3], [4, 48, 192, 107, 146, 3], [4, 64, 139, 3], [4, 193, 58, 3], [4, 74, 27, 167, 3], [4, 74, 64, 87, 131, 3], [4, 74, 117, 177, 3], [4, 74, 135, 105, 61, 3], [4, 74, 37, 166, 3], [4, 77, 99, 3], [4, 78, 82, 192, 3], [4, 82, 160, 3], [4, 82, 85, 77, 192, 3], [4, 82, 85, 168, 192, 3], [4, 82, 43, 192, 3], [4, 82, 116, 192, 3], [4, 85, 9, 3], [4, 85, 8, 157, 3], [4, 85, 41, 3], [4, 85, 55, 24, 3], [4, 85, 55, 34, 3], [4, 85, 57, 2, 156, 192, 137, 3], [4, 85, 83, 3], [4, 85, 86, 46, 181, 165, 48, 3], [4, 85, 111, 129, 152, 3], [4, 85, 129, 68, 3], [4, 85, 138, 86, 158, 33, 3], [4, 85, 12, 141, 147, 165, 75, 157, 3], [4, 85, 151, 62, 56, 3], [4, 85, 164, 3], [4, 85, 173, 72, 3], [4, 85, 111, 77, 192, 159, 3], [4, 85, 51, 10, 3], [4, 85, 48, 111, 95, 89, 3], [4, 85, 48, 111, 100, 89, 3], [4, 85, 52, 10, 3], [4, 85, 53, 5, 96, 3], [4, 85, 67, 84, 3], [4, 85, 95, 192, 85, 97, 192, 3], [4, 85, 107, 176, 3], [4, 85, 128, 3], [4, 85, 129, 17, 89, 3], [4, 85, 144, 67, 91, 6, 89, 3], [4, 85, 149, 26, 145, 3], [4, 89, 48, 111, 98, 15, 45, 165, 99, 3], [4, 89, 109, 31, 188, 192, 3], [4, 93, 81, 153, 163, 3], [4, 105, 106, 191, 3], [4, 109, 102, 192, 3], [4, 110, 107, 189, 52, 189, 3], [4, 112, 119, 74, 108, 84, 192, 3], [4, 126, 143, 103, 176, 87, 158, 63, 3], [4, 127, 23, 3], [4, 133, 58, 3], [4, 178, 64, 120, 3], [4, 148, 142, 3], [4, 153, 29, 114, 194, 3], [4, 155, 99, 169, 3], [4, 156, 192, 141, 104, 3], [4, 157, 90, 192, 3], [4, 162, 122, 21, 3], [4, 170, 165, 172, 3], [4, 171, 117, 94, 171, 115, 94, 3], [4, 178, 11, 188, 192, 3], [4, 179, 136, 3], [4, 181, 192, 48, 3], [4, 181, 192, 155, 81, 3], [4, 181, 192, 48, 3], [4, 181, 48, 192, 161, 3], [4, 181, 48, 192, 174, 22, 3], [4, 181, 71, 3], [4, 181, 73, 192, 123, 193, 30, 3], [4, 181, 193, 121, 113, 3], [4, 181, 192, 174, 3], [4, 182, 167, 92, 3], [4, 183, 192, 62, 3], [4, 184, 35, 132, 192, 62, 3], [4, 185, 192, 3], [4, 186, 192, 14, 3], [4, 186, 192, 41, 3], [4, 186, 192, 47, 3], [4, 192, 19, 3], [4, 192, 66, 3], [4, 192, 180, 3], [4, 192, 48, 89, 3], [4, 192, 48, 16, 1, 85, 48, 111, 29, 3], [4, 192, 107, 5, 101, 1, 153, 162, 118, 3]]\n"
          ]
        }
      ],
      "source": [
        "input_tensor, target_tensor, lang, max_length_inp, max_length_targ = load_dataset(sent_pairs, len(lines))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mRbEyfeiKITs"
      },
      "source": [
        "##**Creating training and validation sets using an 80-20 split**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xTwsYb7u8cx2",
        "outputId": "725a93c6-3c6e-4471-f8ed-46dc9c727344"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(90, 90, 11, 11)"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Creating training and validation sets using an 80-20 split\n",
        "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.1, random_state = 101)\n",
        "\n",
        "# Show length\n",
        "len(input_tensor_train), len(target_tensor_train), len(input_tensor_val), len(target_tensor_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "14eBRMm1dBca",
        "outputId": "267dd814-f65d-43df-fba6-889d61074a4d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[  4, 155,  99, 169,   3,   0,   0,   0,   0,   0],\n",
              "       [  4,  85,  12, 164,   3,   0,   0,   0,   0,   0],\n",
              "       [  4,  64,  13, 139,   3,   0,   0,   0,   0,   0],\n",
              "       [  4,  85,  53,   5,  96,   3,   0,   0,   0,   0],\n",
              "       [  4,  85,  12,   8, 114, 157,   3,   0,   0,   0],\n",
              "       [  4, 181,  73, 192, 124,  59, 193,  30,   3,   0],\n",
              "       [  4,  48, 111, 189,   3,   0,   0,   0,   0,   0],\n",
              "       [  4,  82,  18, 160,   3,   0,   0,   0,   0,   0],\n",
              "       [  4,  85,  67,  84,   3,   0,   0,   0,   0,   0],\n",
              "       [  4,  74, 135,  88, 105,  61,   3,   0,   0,   0],\n",
              "       [  4,  28,  85,  77, 192,   3,   0,   0,   0,   0],\n",
              "       [  4, 181,  48, 192, 161,   3,   0,   0,   0,   0],\n",
              "       [  4,  85, 175, 149,  26, 143, 118,   3,   0,   0],\n",
              "       [  4,  85,  48, 111, 100,  89,   3,   0,   0,   0],\n",
              "       [  4,  85,  48, 111,  95,  89,   3,   0,   0,   0],\n",
              "       [  4, 179, 158, 136,   3,   0,   0,   0,   0,   0],\n",
              "       [  4,  89,  49, 111,  98,  15,  45, 165,  99,   3],\n",
              "       [  4,  25, 176,  59,  99,   3,   0,   0,   0,   0],\n",
              "       [  4,  48, 111, 153,  89, 165, 158,  76,   3,   0],\n",
              "       [  4,  85,  28, 111,  77, 192, 159,   3,   0,   0],\n",
              "       [  4, 192, 107,   5, 101,   1, 153, 162, 118,   3],\n",
              "       [  4,  85,  12, 141, 147, 165,  75, 157,   3,   0],\n",
              "       [  4,  77,  99,   3,   0,   0,   0,   0,   0,   0],\n",
              "       [  4,  82,  28,  85,  77, 192,   3,   0,   0,   0],\n",
              "       [  4,  74, 190,  20,  37, 166,   3,   0,   0,   0],\n",
              "       [  4, 171, 117,  94, 171, 115,  94,   3,   0,   0],\n",
              "       [  4,  82,  28,  85, 168, 192,   3,   0,   0,   0],\n",
              "       [  4,  74,  88, 117, 158, 177,   3,   0,   0,   0],\n",
              "       [  4, 127, 158,  23,   3,   0,   0,   0,   0,   0],\n",
              "       [  4,  85,  12,   9,   3,   0,   0,   0,   0,   0],\n",
              "       [  4,  85,  12,  57,   2, 156, 192, 137,   3,   0],\n",
              "       [  4, 186,  18, 192,  47,   3,   0,   0,   0,   0],\n",
              "       [  4, 192,  18, 180,   3,   0,   0,   0,   0,   0],\n",
              "       [  4,  78,  82,  18, 192,   3,   0,   0,   0,   0],\n",
              "       [  4, 181,  44, 192, 155,  81,   3,   0,   0,   0],\n",
              "       [  4,  85,  12,  55,  24,   3,   0,   0,   0,   0],\n",
              "       [  4,  85, 128,   3,   0,   0,   0,   0,   0,   0],\n",
              "       [  4, 192,  48,  16,   1,  85,  48, 111,  29,   3],\n",
              "       [  4,  69, 193,  58,   3,   0,   0,   0,   0,   0],\n",
              "       [  4, 181,  71,   3,   0,   0,   0,   0,   0,   0],\n",
              "       [  4,  85,  12, 151,  62,  56,   3,   0,   0,   0],\n",
              "       [  4, 157,  88, 141,  90, 114, 192,   3,   0,   0],\n",
              "       [  4, 186,  18, 192,  14,   3,   0,   0,   0,   0],\n",
              "       [  4,  48, 192, 107, 146,   3,   0,   0,   0,   0],\n",
              "       [  4, 134, 178,  64, 120,   3,   0,   0,   0,   0],\n",
              "       [  4, 133, 158,  58,   3,   0,   0,   0,   0,   0],\n",
              "       [  4,  85, 129,  17,  89,   3,   0,   0,   0,   0],\n",
              "       [  4,  74,  27,  26, 167,   3,   0,   0,   0,   0],\n",
              "       [  4, 126, 143, 103, 176,  87, 158,  63,   3,   0],\n",
              "       [  4, 186,  18, 192,  42,   3,   0,   0,   0,   0],\n",
              "       [  4,  85,  48, 111,  10,   3,   0,   0,   0,   0],\n",
              "       [  4,  85,  12, 111, 129, 152,   3,   0,   0,   0],\n",
              "       [  4,  85,  12, 129,  68,   3,   0,   0,   0,   0],\n",
              "       [  4,  85, 144,  67, 165,  91,   6,  89,   3,   0],\n",
              "       [  4,  85,  95, 192,  85,  97, 192,   3,   0,   0],\n",
              "       [  4, 162, 122,  88,  21,   3,   0,   0,   0,   0],\n",
              "       [  4,  40, 192, 125, 154, 140,   3,   0,   0,   0],\n",
              "       [  4, 183,  18, 192,  62,   3,   0,   0,   0,   0],\n",
              "       [  4,  82,  43, 192,   3,   0,   0,   0,   0,   0],\n",
              "       [  4,  85,  12,  83,   3,   0,   0,   0,   0,   0],\n",
              "       [  4, 109, 165, 102, 192,   3,   0,   0,   0,   0],\n",
              "       [  4, 192,  28,  48,  89,   3,   0,   0,   0,   0],\n",
              "       [  4,  18, 192,  60, 166,   3,   0,   0,   0,   0],\n",
              "       [  4,  48, 111,  20, 150,   3,   0,   0,   0,   0],\n",
              "       [  4,  74,  88,  65,  87, 158, 131,   3,   0,   0],\n",
              "       [  4, 156, 192, 141, 104,   3,   0,   0,   0,   0],\n",
              "       [  4,  85,  12, 173,  72,   3,   0,   0,   0,   0],\n",
              "       [  4,  85,  12,  55,  34,   3,   0,   0,   0,   0],\n",
              "       [  4,  89, 175, 109,  32, 188, 192,   3,   0,   0],\n",
              "       [  4,  82, 116,  18, 192,   3,   0,   0,   0,   0],\n",
              "       [  4,  48,  99,   5,  54,   3,   0,   0,   0,   0],\n",
              "       [  4,  85,  52,  10,   3,   0,   0,   0,   0,   0],\n",
              "       [  4, 182, 187, 158, 167,  92,   3,   0,   0,   0],\n",
              "       [  4,  48, 111,  98,  99,  14,   3,   0,   0,   0],\n",
              "       [  4,  36, 193,  70,   3,   0,   0,   0,   0,   0],\n",
              "       [  4, 112, 119,  74, 187, 108,  84, 192,   3,   0],\n",
              "       [  4,  93,  81, 153, 163,   3,   0,   0,   0,   0],\n",
              "       [  4, 185,  18, 192,   3,   0,   0,   0,   0,   0],\n",
              "       [  4, 192,  18,  66,   3,   0,   0,   0,   0,   0],\n",
              "       [  4,  28, 192, 130, 157, 125,   3,   0,   0,   0],\n",
              "       [  4,  85,  12, 138,  86, 158,  33,   3,   0,   0],\n",
              "       [  4, 178,  18,  11, 188, 192,   3,   0,   0,   0],\n",
              "       [  4,  48, 111,   7,  81,   3,   0,   0,   0,   0],\n",
              "       [  4, 170, 165, 172,   3,   0,   0,   0,   0,   0],\n",
              "       [  4, 181, 192, 174,   3,   0,   0,   0,   0,   0],\n",
              "       [  4, 110, 107, 165, 189,  52, 189,   3,   0,   0],\n",
              "       [  4, 153,  29, 114, 194,   3,   0,   0,   0,   0],\n",
              "       [  4, 181,  48, 192,  48,   3,   0,   0,   0,   0],\n",
              "       [  4,  48, 111,  84,  99,   3,   0,   0,   0,   0],\n",
              "       [  4, 192,  18,  19,   3,   0,   0,   0,   0,   0]], dtype=int32)"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "input_tensor_train\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0mxwf-8qdlhI",
        "outputId": "c3d9b750-d735-4232-ff59-7fdc647b841b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[  4, 155,  99, 169,   3,   0,   0,   0,   0,   0],\n",
              "       [  4,  85, 164,   3,   0,   0,   0,   0,   0,   0],\n",
              "       [  4,  64, 139,   3,   0,   0,   0,   0,   0,   0],\n",
              "       [  4,  85,  53,   5,  96,   3,   0,   0,   0,   0],\n",
              "       [  4,  85,   8, 157,   3,   0,   0,   0,   0,   0],\n",
              "       [  4, 181,  73, 192, 123, 193,  30,   3,   0,   0],\n",
              "       [  4,  48, 111, 189,   3,   0,   0,   0,   0,   0],\n",
              "       [  4,  82, 160,   3,   0,   0,   0,   0,   0,   0],\n",
              "       [  4,  85,  67,  84,   3,   0,   0,   0,   0,   0],\n",
              "       [  4,  74, 135, 105,  61,   3,   0,   0,   0,   0],\n",
              "       [  4,  85,  77, 192,   3,   0,   0,   0,   0,   0],\n",
              "       [  4, 181,  48, 192, 161,   3,   0,   0,   0,   0],\n",
              "       [  4,  85, 149,  26, 145,   3,   0,   0,   0,   0],\n",
              "       [  4,  85,  48, 111, 100,  89,   3,   0,   0,   0],\n",
              "       [  4,  85,  48, 111,  95,  89,   3,   0,   0,   0],\n",
              "       [  4, 179, 136,   3,   0,   0,   0,   0,   0,   0],\n",
              "       [  4,  89,  48, 111,  98,  15,  45, 165,  99,   3],\n",
              "       [  4,  25, 176,  99,   3,   0,   0,   0,   0,   0],\n",
              "       [  4,  48, 111, 153,  89,  76,   3,   0,   0,   0],\n",
              "       [  4,  85, 111,  77, 192, 159,   3,   0,   0,   0],\n",
              "       [  4, 192, 107,   5, 101,   1, 153, 162, 118,   3],\n",
              "       [  4,  85,  12, 141, 147, 165,  75, 157,   3,   0],\n",
              "       [  4,  77,  99,   3,   0,   0,   0,   0,   0,   0],\n",
              "       [  4,  82,  85,  77, 192,   3,   0,   0,   0,   0],\n",
              "       [  4,  74,  37, 166,   3,   0,   0,   0,   0,   0],\n",
              "       [  4, 171, 117,  94, 171, 115,  94,   3,   0,   0],\n",
              "       [  4,  82,  85, 168, 192,   3,   0,   0,   0,   0],\n",
              "       [  4,  74, 117, 177,   3,   0,   0,   0,   0,   0],\n",
              "       [  4, 127,  23,   3,   0,   0,   0,   0,   0,   0],\n",
              "       [  4,  85,   9,   3,   0,   0,   0,   0,   0,   0],\n",
              "       [  4,  85,  57,   2, 156, 192, 137,   3,   0,   0],\n",
              "       [  4, 186, 192,  47,   3,   0,   0,   0,   0,   0],\n",
              "       [  4, 192, 180,   3,   0,   0,   0,   0,   0,   0],\n",
              "       [  4,  78,  82, 192,   3,   0,   0,   0,   0,   0],\n",
              "       [  4, 181, 192, 155,  81,   3,   0,   0,   0,   0],\n",
              "       [  4,  85,  55,  24,   3,   0,   0,   0,   0,   0],\n",
              "       [  4,  85, 128,   3,   0,   0,   0,   0,   0,   0],\n",
              "       [  4, 192,  48,  16,   1,  85,  48, 111,  29,   3],\n",
              "       [  4, 193,  58,   3,   0,   0,   0,   0,   0,   0],\n",
              "       [  4, 181,  71,   3,   0,   0,   0,   0,   0,   0],\n",
              "       [  4,  85, 151,  62,  56,   3,   0,   0,   0,   0],\n",
              "       [  4, 157,  90, 192,   3,   0,   0,   0,   0,   0],\n",
              "       [  4, 186, 192,  14,   3,   0,   0,   0,   0,   0],\n",
              "       [  4,  48, 192, 107, 146,   3,   0,   0,   0,   0],\n",
              "       [  4, 178,  64, 120,   3,   0,   0,   0,   0,   0],\n",
              "       [  4, 133,  58,   3,   0,   0,   0,   0,   0,   0],\n",
              "       [  4,  85, 129,  17,  89,   3,   0,   0,   0,   0],\n",
              "       [  4,  74,  27, 167,   3,   0,   0,   0,   0,   0],\n",
              "       [  4, 126, 143, 103, 176,  87, 158,  63,   3,   0],\n",
              "       [  4, 186, 192,  41,   3,   0,   0,   0,   0,   0],\n",
              "       [  4,  85,  51,  10,   3,   0,   0,   0,   0,   0],\n",
              "       [  4,  85, 111, 129, 152,   3,   0,   0,   0,   0],\n",
              "       [  4,  85, 129,  68,   3,   0,   0,   0,   0,   0],\n",
              "       [  4,  85, 144,  67,  91,   6,  89,   3,   0,   0],\n",
              "       [  4,  85,  95, 192,  85,  97, 192,   3,   0,   0],\n",
              "       [  4, 162, 122,  21,   3,   0,   0,   0,   0,   0],\n",
              "       [  4, 192, 125, 154, 140,   3,   0,   0,   0,   0],\n",
              "       [  4, 183, 192,  62,   3,   0,   0,   0,   0,   0],\n",
              "       [  4,  82,  43, 192,   3,   0,   0,   0,   0,   0],\n",
              "       [  4,  85,  83,   3,   0,   0,   0,   0,   0,   0],\n",
              "       [  4, 109, 102, 192,   3,   0,   0,   0,   0,   0],\n",
              "       [  4, 192,  48,  89,   3,   0,   0,   0,   0,   0],\n",
              "       [  4, 192,  60, 166,   3,   0,   0,   0,   0,   0],\n",
              "       [  4,  51,  20, 150,   3,   0,   0,   0,   0,   0],\n",
              "       [  4,  74,  64,  87, 131,   3,   0,   0,   0,   0],\n",
              "       [  4, 156, 192, 141, 104,   3,   0,   0,   0,   0],\n",
              "       [  4,  85, 173,  72,   3,   0,   0,   0,   0,   0],\n",
              "       [  4,  85,  55,  34,   3,   0,   0,   0,   0,   0],\n",
              "       [  4,  89, 109,  31, 188, 192,   3,   0,   0,   0],\n",
              "       [  4,  82, 116, 192,   3,   0,   0,   0,   0,   0],\n",
              "       [  4,  48,  99,  54,   3,   0,   0,   0,   0,   0],\n",
              "       [  4,  85,  52,  10,   3,   0,   0,   0,   0,   0],\n",
              "       [  4, 182, 167,  92,   3,   0,   0,   0,   0,   0],\n",
              "       [  4,  48, 111,  98,  99,  14,   3,   0,   0,   0],\n",
              "       [  4,  36, 192,  70,   3,   0,   0,   0,   0,   0],\n",
              "       [  4, 112, 119,  74, 108,  84, 192,   3,   0,   0],\n",
              "       [  4,  93,  81, 153, 163,   3,   0,   0,   0,   0],\n",
              "       [  4, 185, 192,   3,   0,   0,   0,   0,   0,   0],\n",
              "       [  4, 192,  66,   3,   0,   0,   0,   0,   0,   0],\n",
              "       [  4, 192, 130, 125,   3,   0,   0,   0,   0,   0],\n",
              "       [  4,  85, 138,  86, 158,  33,   3,   0,   0,   0],\n",
              "       [  4, 178,  11, 188, 192,   3,   0,   0,   0,   0],\n",
              "       [  4,  51,   7,  81,   3,   0,   0,   0,   0,   0],\n",
              "       [  4, 170, 165, 172,   3,   0,   0,   0,   0,   0],\n",
              "       [  4, 181, 192, 174,   3,   0,   0,   0,   0,   0],\n",
              "       [  4, 110, 107, 189,  52, 189,   3,   0,   0,   0],\n",
              "       [  4, 153,  29, 114, 194,   3,   0,   0,   0,   0],\n",
              "       [  4, 181, 192,  48,   3,   0,   0,   0,   0,   0],\n",
              "       [  4,  51,  84,  99,   3,   0,   0,   0,   0,   0],\n",
              "       [  4, 192,  19,   3,   0,   0,   0,   0,   0,   0]], dtype=int32)"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "target_tensor_train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HMSEPJrWKoPQ"
      },
      "source": [
        "##**Create a tf.data dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "id": "q8Q-cS278cvx"
      },
      "outputs": [],
      "source": [
        "BUFFER_SIZE = len(input_tensor_train)\n",
        "BATCH_SIZE = 12\n",
        "N_BATCH = BUFFER_SIZE//BATCH_SIZE\n",
        "embedding_dim = 190\n",
        "units =   64\n",
        "vocab_inp_size = len(lang.word2idx)\n",
        "vocab_tar_size = len(lang.word2idx)\n",
        "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "moWKTzhhQAhr",
        "outputId": "63e06306-9128-4e50-cf9c-b74c51091eac"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(TensorShape([12, 10]), TensorShape([12, 10]))"
            ]
          },
          "execution_count": 96,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "example_input_batch, example_target_batch = next(iter(dataset))\n",
        "example_input_batch.shape, example_target_batch.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mn8HdsApLIYe"
      },
      "source": [
        "##**Define the encoder and decoder network**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "id": "obkCjaN_-ZDM"
      },
      "outputs": [],
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.batch_sz = batch_sz\n",
        "        self.enc_units = enc_units\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "        self.gru = tf.keras.layers.GRU(self.enc_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_activation='sigmoid',\n",
        "                                   recurrent_initializer='glorot_uniform',\n",
        "                                       dropout=0.4)\n",
        "\n",
        "    def call(self, x, hidden):\n",
        "        x = self.embedding(x)\n",
        "        output, state = self.gru(x, initial_state = hidden)\n",
        "        return output, state\n",
        "\n",
        "    def initialize_hidden_state(self):\n",
        "        return tf.zeros((self.batch_sz, self.enc_units))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TZIwXwv88crd",
        "outputId": "b29ea1bd-07f8-4a81-d9f4-dc3138d33331"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Encoder output shape: (batch size, sequence length, units) (12, 10, 64)\n",
            "Encoder Hidden state shape: (batch size, units) (12, 64)\n"
          ]
        }
      ],
      "source": [
        "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
        "\n",
        "# sample input\n",
        "sample_hidden = encoder.initialize_hidden_state()\n",
        "sample_output, sample_hidden = encoder(example_input_batch, sample_hidden)\n",
        "print('Encoder output shape: (batch size, sequence length, units)', sample_output.shape)\n",
        "print('Encoder Hidden state shape: (batch size, units)', sample_hidden.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "id": "E1Lw4BPHBB_n"
      },
      "outputs": [],
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.batch_sz = batch_sz\n",
        "        self.dec_units = dec_units\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "        self.gru = tf.keras.layers.GRU(self.dec_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform',\n",
        "                                       dropout=0.4)\n",
        "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "        # used for attention\n",
        "        self.W1 = tf.keras.layers.Dense(self.dec_units)\n",
        "        self.W2 = tf.keras.layers.Dense(self.dec_units)\n",
        "        self.V = tf.keras.layers.Dense(1)\n",
        "\n",
        "    def call(self, x, hidden, enc_output):\n",
        "\n",
        "        hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
        "\n",
        "        # score shape == (batch_size, max_length, 1)\n",
        "        # we get 1 at the last axis because we are applying tanh(FC(EO) + FC(H)) to self.V\n",
        "        score = self.V(tf.nn.tanh(self.W1(enc_output) + self.W2(hidden_with_time_axis)))\n",
        "\n",
        "        # attention_weights shape == (batch_size, max_length, 1)\n",
        "        attention_weights = tf.nn.softmax(score, axis=1)\n",
        "\n",
        "        # context_vector shape after sum == (batch_size, hidden_size)\n",
        "        context_vector = attention_weights * enc_output\n",
        "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "\n",
        "        # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
        "        x = self.embedding(x)\n",
        "\n",
        "        # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
        "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
        "\n",
        "        # passing the concatenated vector to the GRU\n",
        "        output, state = self.gru(x)\n",
        "\n",
        "        # output shape == (batch_size * 1, hidden_size)\n",
        "        output = tf.reshape(output, (-1, output.shape[2]))\n",
        "\n",
        "        # output shape == (batch_size * 1, vocab)\n",
        "        x = self.fc(output)\n",
        "\n",
        "        return x, state, attention_weights\n",
        "\n",
        "    def initialize_hidden_state(self):\n",
        "        return tf.zeros((self.batch_sz, self.dec_units))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uhqqP4K08cfF",
        "outputId": "90ff693e-cf75-43df-e07c-406cc5608880"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Decoder output shape: (batch_size, vocab size) (12, 195)\n"
          ]
        }
      ],
      "source": [
        "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)\n",
        "\n",
        "sample_decoder_output, _, _ = decoder(tf.random.uniform((BATCH_SIZE, 1)),\n",
        "                                      sample_hidden, sample_output)\n",
        "\n",
        "print('Decoder output shape: (batch_size, vocab size)', sample_decoder_output.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3oAyHIoXLxKe"
      },
      "source": [
        "## **Define the optimizer and the loss function**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "id": "AZZEm6LN8cb8"
      },
      "outputs": [],
      "source": [
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True,\n",
        "                                                            reduction='none')\n",
        "\n",
        "\n",
        "def loss_function(real, pred):\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  loss_ = loss_object(real, pred)\n",
        "\n",
        "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "  loss_ *= mask\n",
        "\n",
        "  return tf.reduce_mean(loss_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QprXJ9h1L5Yx"
      },
      "source": [
        "## **Checkpoints (Object-based saving)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "id": "IgLcJvMKL4-8"
      },
      "outputs": [],
      "source": [
        "checkpoint_dir = '/content/drive/MyDrive/training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
        "                                 encoder=encoder,\n",
        "                                 decoder=decoder)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "50jlHQRiMG-O"
      },
      "source": [
        "## **Training**\n",
        "\n",
        "1. Pass the *input* through the *encoder* which return *encoder output* and the *encoder hidden state*.\n",
        "2. The encoder output, encoder hidden state and the decoder input (which is the *start token*) is passed to the decoder.\n",
        "3. The decoder returns the *predictions* and the *decoder hidden state*.\n",
        "4. The decoder hidden state is then passed back into the model and the predictions are used to calculate the loss.\n",
        "5. Use *teacher forcing* to decide the next input to the decoder.\n",
        "6. *Teacher forcing* is the technique where the *target word* is passed as the *next input* to the decoder.\n",
        "7. The final step is to calculate the gradients and apply it to the optimizer and backpropagate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z6QeHuRCSGW-",
        "outputId": "1a7bebc7-12e2-4cb7-d23b-9d403e9ab15e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 Batch 0 Loss 2.2863\n",
            "Epoch 1 Loss 2.4235\n",
            "Time taken for 1 epoch 2.710221767425537 sec\n",
            "\n",
            "Epoch 2 Batch 0 Loss 2.0462\n",
            "Epoch 2 Loss 2.4581\n",
            "Time taken for 1 epoch 2.6727254390716553 sec\n",
            "\n",
            "Epoch 3 Batch 0 Loss 2.4429\n",
            "Epoch 3 Loss 2.3690\n",
            "Time taken for 1 epoch 5.188555479049683 sec\n",
            "\n",
            "Epoch 4 Batch 0 Loss 2.6507\n",
            "Epoch 4 Loss 2.1018\n",
            "Time taken for 1 epoch 2.661336660385132 sec\n",
            "\n",
            "Epoch 5 Batch 0 Loss 1.9962\n",
            "Epoch 5 Loss 1.9634\n",
            "Time taken for 1 epoch 2.197526216506958 sec\n",
            "\n",
            "Epoch 6 Batch 0 Loss 2.0301\n",
            "Epoch 6 Loss 1.8708\n",
            "Time taken for 1 epoch 2.6781883239746094 sec\n",
            "\n",
            "Epoch 7 Batch 0 Loss 1.8038\n",
            "Epoch 7 Loss 1.8857\n",
            "Time taken for 1 epoch 2.682539939880371 sec\n",
            "\n",
            "Epoch 8 Batch 0 Loss 1.8232\n",
            "Epoch 8 Loss 1.8475\n",
            "Time taken for 1 epoch 2.6559391021728516 sec\n",
            "\n",
            "Epoch 9 Batch 0 Loss 1.7753\n",
            "Epoch 9 Loss 1.8255\n",
            "Time taken for 1 epoch 2.188988447189331 sec\n",
            "\n",
            "Epoch 10 Batch 0 Loss 1.7699\n",
            "Epoch 10 Loss 1.7965\n",
            "Time taken for 1 epoch 2.2081196308135986 sec\n",
            "\n",
            "Epoch 11 Batch 0 Loss 1.9457\n",
            "Epoch 11 Loss 1.7721\n",
            "Time taken for 1 epoch 2.3464319705963135 sec\n",
            "\n",
            "Epoch 12 Batch 0 Loss 1.8245\n",
            "Epoch 12 Loss 1.7452\n",
            "Time taken for 1 epoch 5.2128355503082275 sec\n",
            "\n",
            "Epoch 13 Batch 0 Loss 2.1488\n",
            "Epoch 13 Loss 1.6939\n",
            "Time taken for 1 epoch 2.6220450401306152 sec\n",
            "\n",
            "Epoch 14 Batch 0 Loss 1.8269\n",
            "Epoch 14 Loss 1.6336\n",
            "Time taken for 1 epoch 2.1405484676361084 sec\n",
            "\n",
            "Epoch 15 Batch 0 Loss 1.4543\n",
            "Epoch 15 Loss 1.6384\n",
            "Time taken for 1 epoch 2.2653462886810303 sec\n",
            "\n",
            "Epoch 16 Batch 0 Loss 1.6848\n",
            "Epoch 16 Loss 1.5993\n",
            "Time taken for 1 epoch 3.1233408451080322 sec\n",
            "\n",
            "Epoch 17 Batch 0 Loss 1.4215\n",
            "Epoch 17 Loss 1.5549\n",
            "Time taken for 1 epoch 2.208120584487915 sec\n",
            "\n",
            "Epoch 18 Batch 0 Loss 1.2962\n",
            "Epoch 18 Loss 1.5269\n",
            "Time taken for 1 epoch 2.6275558471679688 sec\n",
            "\n",
            "Epoch 19 Batch 0 Loss 1.5568\n",
            "Epoch 19 Loss 1.5113\n",
            "Time taken for 1 epoch 2.6485278606414795 sec\n",
            "\n",
            "Epoch 20 Batch 0 Loss 1.5697\n",
            "Epoch 20 Loss 1.4677\n",
            "Time taken for 1 epoch 2.2236733436584473 sec\n",
            "\n",
            "Epoch 21 Batch 0 Loss 1.5867\n",
            "Epoch 21 Loss 1.4359\n",
            "Time taken for 1 epoch 5.234410047531128 sec\n",
            "\n",
            "Epoch 22 Batch 0 Loss 1.6301\n",
            "Epoch 22 Loss 1.4195\n",
            "Time taken for 1 epoch 2.1841115951538086 sec\n",
            "\n",
            "Epoch 23 Batch 0 Loss 1.5029\n",
            "Epoch 23 Loss 1.3985\n",
            "Time taken for 1 epoch 2.6496782302856445 sec\n",
            "\n",
            "Epoch 24 Batch 0 Loss 1.3898\n",
            "Epoch 24 Loss 1.3659\n",
            "Time taken for 1 epoch 2.628396511077881 sec\n",
            "\n",
            "Epoch 25 Batch 0 Loss 1.2616\n",
            "Epoch 25 Loss 1.3080\n",
            "Time taken for 1 epoch 5.198839426040649 sec\n",
            "\n",
            "Epoch 26 Batch 0 Loss 1.0574\n",
            "Epoch 26 Loss 1.2928\n",
            "Time taken for 1 epoch 2.624382734298706 sec\n",
            "\n",
            "Epoch 27 Batch 0 Loss 1.1545\n",
            "Epoch 27 Loss 1.2680\n",
            "Time taken for 1 epoch 2.1643431186676025 sec\n",
            "\n",
            "Epoch 28 Batch 0 Loss 1.3438\n",
            "Epoch 28 Loss 1.2653\n",
            "Time taken for 1 epoch 2.186884880065918 sec\n",
            "\n",
            "Epoch 29 Batch 0 Loss 1.5044\n",
            "Epoch 29 Loss 1.2006\n",
            "Time taken for 1 epoch 5.180207014083862 sec\n",
            "\n",
            "Epoch 30 Batch 0 Loss 1.1057\n",
            "Epoch 30 Loss 1.1991\n",
            "Time taken for 1 epoch 2.2199392318725586 sec\n",
            "\n",
            "Epoch 31 Batch 0 Loss 1.1784\n",
            "Epoch 31 Loss 1.1782\n",
            "Time taken for 1 epoch 2.1556875705718994 sec\n",
            "\n",
            "Epoch 32 Batch 0 Loss 1.3258\n",
            "Epoch 32 Loss 1.1635\n",
            "Time taken for 1 epoch 2.20298171043396 sec\n",
            "\n",
            "Epoch 33 Batch 0 Loss 1.0579\n",
            "Epoch 33 Loss 1.1376\n",
            "Time taken for 1 epoch 2.4433414936065674 sec\n",
            "\n",
            "Epoch 34 Batch 0 Loss 0.9619\n",
            "Epoch 34 Loss 1.1008\n",
            "Time taken for 1 epoch 2.855618953704834 sec\n",
            "\n",
            "Epoch 35 Batch 0 Loss 1.0432\n",
            "Epoch 35 Loss 1.0472\n",
            "Time taken for 1 epoch 2.220344305038452 sec\n",
            "\n",
            "Epoch 36 Batch 0 Loss 0.9788\n",
            "Epoch 36 Loss 1.0550\n",
            "Time taken for 1 epoch 2.6356430053710938 sec\n",
            "\n",
            "Epoch 37 Batch 0 Loss 0.8619\n",
            "Epoch 37 Loss 0.9999\n",
            "Time taken for 1 epoch 2.2518396377563477 sec\n",
            "\n",
            "Epoch 38 Batch 0 Loss 1.0273\n",
            "Epoch 38 Loss 0.9727\n",
            "Time taken for 1 epoch 2.6481993198394775 sec\n",
            "\n",
            "Epoch 39 Batch 0 Loss 1.0493\n",
            "Epoch 39 Loss 0.9551\n",
            "Time taken for 1 epoch 2.9337048530578613 sec\n",
            "\n",
            "Epoch 40 Batch 0 Loss 0.8381\n",
            "Epoch 40 Loss 0.9187\n",
            "Time taken for 1 epoch 2.256240129470825 sec\n",
            "\n",
            "Epoch 41 Batch 0 Loss 0.9437\n",
            "Epoch 41 Loss 0.9125\n",
            "Time taken for 1 epoch 2.6600115299224854 sec\n",
            "\n",
            "Epoch 42 Batch 0 Loss 0.7830\n",
            "Epoch 42 Loss 0.8830\n",
            "Time taken for 1 epoch 2.6289515495300293 sec\n",
            "\n",
            "Epoch 43 Batch 0 Loss 0.9136\n",
            "Epoch 43 Loss 0.8577\n",
            "Time taken for 1 epoch 2.6753458976745605 sec\n",
            "\n",
            "Epoch 44 Batch 0 Loss 0.6984\n",
            "Epoch 44 Loss 0.8223\n",
            "Time taken for 1 epoch 5.1839470863342285 sec\n",
            "\n",
            "Epoch 45 Batch 0 Loss 0.7547\n",
            "Epoch 45 Loss 0.8023\n",
            "Time taken for 1 epoch 2.674443006515503 sec\n",
            "\n",
            "Epoch 46 Batch 0 Loss 0.7347\n",
            "Epoch 46 Loss 0.7847\n",
            "Time taken for 1 epoch 2.2005152702331543 sec\n",
            "\n",
            "Epoch 47 Batch 0 Loss 0.5949\n",
            "Epoch 47 Loss 0.7506\n",
            "Time taken for 1 epoch 2.6810154914855957 sec\n",
            "\n",
            "Epoch 48 Batch 0 Loss 0.7933\n",
            "Epoch 48 Loss 0.7334\n",
            "Time taken for 1 epoch 5.190540790557861 sec\n",
            "\n",
            "Epoch 49 Batch 0 Loss 0.6760\n",
            "Epoch 49 Loss 0.7181\n",
            "Time taken for 1 epoch 2.651934862136841 sec\n",
            "\n",
            "Epoch 50 Batch 0 Loss 0.6852\n",
            "Epoch 50 Loss 0.6996\n",
            "Time taken for 1 epoch 2.1877248287200928 sec\n",
            "\n",
            "Epoch 51 Batch 0 Loss 0.6565\n",
            "Epoch 51 Loss 0.6726\n",
            "Time taken for 1 epoch 2.645261526107788 sec\n",
            "\n",
            "Epoch 52 Batch 0 Loss 0.6369\n",
            "Epoch 52 Loss 0.6487\n",
            "Time taken for 1 epoch 2.826446533203125 sec\n",
            "\n",
            "Epoch 53 Batch 0 Loss 0.5242\n",
            "Epoch 53 Loss 0.6439\n",
            "Time taken for 1 epoch 2.2046704292297363 sec\n",
            "\n",
            "Epoch 54 Batch 0 Loss 0.6970\n",
            "Epoch 54 Loss 0.6157\n",
            "Time taken for 1 epoch 2.678401470184326 sec\n",
            "\n",
            "Epoch 55 Batch 0 Loss 0.5260\n",
            "Epoch 55 Loss 0.5910\n",
            "Time taken for 1 epoch 2.6275851726531982 sec\n",
            "\n",
            "Epoch 56 Batch 0 Loss 0.6790\n",
            "Epoch 56 Loss 0.5745\n",
            "Time taken for 1 epoch 2.6646995544433594 sec\n",
            "\n",
            "Epoch 57 Batch 0 Loss 0.4968\n",
            "Epoch 57 Loss 0.5549\n",
            "Time taken for 1 epoch 2.801307439804077 sec\n",
            "\n",
            "Epoch 58 Batch 0 Loss 0.5378\n",
            "Epoch 58 Loss 0.5507\n",
            "Time taken for 1 epoch 2.6470603942871094 sec\n",
            "\n",
            "Epoch 59 Batch 0 Loss 0.5691\n",
            "Epoch 59 Loss 0.5349\n",
            "Time taken for 1 epoch 2.225660562515259 sec\n",
            "\n",
            "Epoch 60 Batch 0 Loss 0.4480\n",
            "Epoch 60 Loss 0.5182\n",
            "Time taken for 1 epoch 2.668578624725342 sec\n",
            "\n",
            "Epoch 61 Batch 0 Loss 0.5102\n",
            "Epoch 61 Loss 0.4964\n",
            "Time taken for 1 epoch 2.6599080562591553 sec\n",
            "\n",
            "Epoch 62 Batch 0 Loss 0.4557\n",
            "Epoch 62 Loss 0.4828\n",
            "Time taken for 1 epoch 2.704331636428833 sec\n",
            "\n",
            "Epoch 63 Batch 0 Loss 0.5063\n",
            "Epoch 63 Loss 0.4730\n",
            "Time taken for 1 epoch 2.1904850006103516 sec\n",
            "\n",
            "Epoch 64 Batch 0 Loss 0.4795\n",
            "Epoch 64 Loss 0.4605\n",
            "Time taken for 1 epoch 2.190992593765259 sec\n",
            "\n",
            "Epoch 65 Batch 0 Loss 0.4374\n",
            "Epoch 65 Loss 0.4485\n",
            "Time taken for 1 epoch 2.648308038711548 sec\n",
            "\n",
            "Epoch 66 Batch 0 Loss 0.3936\n",
            "Epoch 66 Loss 0.4295\n",
            "Time taken for 1 epoch 2.642751693725586 sec\n",
            "\n",
            "Epoch 67 Batch 0 Loss 0.4174\n",
            "Epoch 67 Loss 0.4207\n",
            "Time taken for 1 epoch 2.8324196338653564 sec\n",
            "\n",
            "Epoch 68 Batch 0 Loss 0.3990\n",
            "Epoch 68 Loss 0.4097\n",
            "Time taken for 1 epoch 2.62324595451355 sec\n",
            "\n",
            "Epoch 69 Batch 0 Loss 0.3955\n",
            "Epoch 69 Loss 0.3973\n",
            "Time taken for 1 epoch 2.679638624191284 sec\n",
            "\n",
            "Epoch 70 Batch 0 Loss 0.4211\n",
            "Epoch 70 Loss 0.3865\n",
            "Time taken for 1 epoch 5.209664583206177 sec\n",
            "\n",
            "Epoch 71 Batch 0 Loss 0.3688\n",
            "Epoch 71 Loss 0.3667\n",
            "Time taken for 1 epoch 2.536771059036255 sec\n",
            "\n",
            "Epoch 72 Batch 0 Loss 0.3964\n",
            "Epoch 72 Loss 0.3565\n",
            "Time taken for 1 epoch 2.6294236183166504 sec\n",
            "\n",
            "Epoch 73 Batch 0 Loss 0.3226\n",
            "Epoch 73 Loss 0.3439\n",
            "Time taken for 1 epoch 2.2325990200042725 sec\n",
            "\n",
            "Epoch 74 Batch 0 Loss 0.3664\n",
            "Epoch 74 Loss 0.3354\n",
            "Time taken for 1 epoch 2.4341750144958496 sec\n",
            "\n",
            "Epoch 75 Batch 0 Loss 0.3108\n",
            "Epoch 75 Loss 0.3261\n",
            "Time taken for 1 epoch 5.185086727142334 sec\n",
            "\n",
            "Epoch 76 Batch 0 Loss 0.3230\n",
            "Epoch 76 Loss 0.3199\n",
            "Time taken for 1 epoch 2.253905773162842 sec\n",
            "\n",
            "Epoch 77 Batch 0 Loss 0.2919\n",
            "Epoch 77 Loss 0.3159\n",
            "Time taken for 1 epoch 2.6274807453155518 sec\n",
            "\n",
            "Epoch 78 Batch 0 Loss 0.3192\n",
            "Epoch 78 Loss 0.3066\n",
            "Time taken for 1 epoch 2.6556222438812256 sec\n",
            "\n",
            "Epoch 79 Batch 0 Loss 0.3761\n",
            "Epoch 79 Loss 0.2916\n",
            "Time taken for 1 epoch 5.185724258422852 sec\n",
            "\n",
            "Epoch 80 Batch 0 Loss 0.3031\n",
            "Epoch 80 Loss 0.2814\n",
            "Time taken for 1 epoch 2.648320436477661 sec\n",
            "\n",
            "Epoch 81 Batch 0 Loss 0.3343\n",
            "Epoch 81 Loss 0.2769\n",
            "Time taken for 1 epoch 2.6217031478881836 sec\n",
            "\n",
            "Epoch 82 Batch 0 Loss 0.2578\n",
            "Epoch 82 Loss 0.2635\n",
            "Time taken for 1 epoch 2.6578547954559326 sec\n",
            "\n",
            "Epoch 83 Batch 0 Loss 0.2348\n",
            "Epoch 83 Loss 0.2522\n",
            "Time taken for 1 epoch 3.1849236488342285 sec\n",
            "\n",
            "Epoch 84 Batch 0 Loss 0.2607\n",
            "Epoch 84 Loss 0.2464\n",
            "Time taken for 1 epoch 2.2055885791778564 sec\n",
            "\n",
            "Epoch 85 Batch 0 Loss 0.2433\n",
            "Epoch 85 Loss 0.2419\n",
            "Time taken for 1 epoch 2.6239137649536133 sec\n",
            "\n",
            "Epoch 86 Batch 0 Loss 0.2318\n",
            "Epoch 86 Loss 0.2289\n",
            "Time taken for 1 epoch 2.1716086864471436 sec\n",
            "\n",
            "Epoch 87 Batch 0 Loss 0.2209\n",
            "Epoch 87 Loss 0.2291\n",
            "Time taken for 1 epoch 2.224177122116089 sec\n",
            "\n",
            "Epoch 88 Batch 0 Loss 0.2509\n",
            "Epoch 88 Loss 0.2191\n",
            "Time taken for 1 epoch 5.193053960800171 sec\n",
            "\n",
            "Epoch 89 Batch 0 Loss 0.1727\n",
            "Epoch 89 Loss 0.2086\n",
            "Time taken for 1 epoch 2.6547672748565674 sec\n",
            "\n",
            "Epoch 90 Batch 0 Loss 0.2133\n",
            "Epoch 90 Loss 0.2003\n",
            "Time taken for 1 epoch 2.259429931640625 sec\n",
            "\n",
            "Epoch 91 Batch 0 Loss 0.1755\n",
            "Epoch 91 Loss 0.1983\n",
            "Time taken for 1 epoch 2.6526877880096436 sec\n",
            "\n",
            "Epoch 92 Batch 0 Loss 0.1579\n",
            "Epoch 92 Loss 0.1928\n",
            "Time taken for 1 epoch 2.900686025619507 sec\n",
            "\n",
            "Epoch 93 Batch 0 Loss 0.1987\n",
            "Epoch 93 Loss 0.1870\n",
            "Time taken for 1 epoch 2.6569831371307373 sec\n",
            "\n",
            "Epoch 94 Batch 0 Loss 0.1740\n",
            "Epoch 94 Loss 0.1807\n",
            "Time taken for 1 epoch 2.274043321609497 sec\n",
            "\n",
            "Epoch 95 Batch 0 Loss 0.1937\n",
            "Epoch 95 Loss 0.1741\n",
            "Time taken for 1 epoch 2.652919292449951 sec\n",
            "\n",
            "Epoch 96 Batch 0 Loss 0.1649\n",
            "Epoch 96 Loss 0.1697\n",
            "Time taken for 1 epoch 2.6251699924468994 sec\n",
            "\n",
            "Epoch 97 Batch 0 Loss 0.1796\n",
            "Epoch 97 Loss 0.1659\n",
            "Time taken for 1 epoch 5.211039066314697 sec\n",
            "\n",
            "Epoch 98 Batch 0 Loss 0.1533\n",
            "Epoch 98 Loss 0.1558\n",
            "Time taken for 1 epoch 2.2802066802978516 sec\n",
            "\n",
            "Epoch 99 Batch 0 Loss 0.1622\n",
            "Epoch 99 Loss 0.1572\n",
            "Time taken for 1 epoch 2.2248964309692383 sec\n",
            "\n",
            "Epoch 100 Batch 0 Loss 0.1657\n",
            "Epoch 100 Loss 0.1477\n",
            "Time taken for 1 epoch 2.2990071773529053 sec\n",
            "\n"
          ]
        }
      ],
      "source": [
        "EPOCHS = 100\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    start = time.time()\n",
        "\n",
        "    hidden = encoder.initialize_hidden_state()\n",
        "    total_loss = 0\n",
        "\n",
        "    for (batch, (inp, targ)) in enumerate(dataset):\n",
        "        loss = 0\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            enc_output, enc_hidden = encoder(inp, hidden)\n",
        "\n",
        "            dec_hidden = enc_hidden\n",
        "\n",
        "            dec_input = tf.expand_dims([lang.word2idx['<start>']] * BATCH_SIZE, 1)\n",
        "\n",
        "            # Teacher forcing - feeding the target as the next input\n",
        "            for t in range(1, targ.shape[1]):\n",
        "                # passing enc_output to the decoder\n",
        "                predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
        "                loss += loss_function(targ[:, t], predictions)\n",
        "\n",
        "                # using teacher forcing\n",
        "                dec_input = tf.expand_dims(targ[:, t], 1)\n",
        "\n",
        "        batch_loss = (loss / int(targ.shape[1]))\n",
        "\n",
        "        total_loss += batch_loss\n",
        "\n",
        "        variables = encoder.variables + decoder.variables\n",
        "\n",
        "        gradients = tape.gradient(loss, variables)\n",
        "\n",
        "        optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "        if batch % 100 == 0:\n",
        "            print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                                         batch,\n",
        "                                                         batch_loss.numpy()))\n",
        "    # saving (checkpoint) the model every epoch\n",
        "    checkpoint.save(file_prefix=checkpoint_prefix)\n",
        "\n",
        "    print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                        total_loss / N_BATCH))\n",
        "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L42AcfdgMhk3"
      },
      "source": [
        "##**Inference and Testing**\n",
        "* The evaluate function is similar to the training loop, except we don't use *teacher forcing* here. The input to the decoder at each time step is its previous predictions along with the hidden state and the encoder output.\n",
        "* Stop predicting when the model predicts the *end token*.\n",
        "* And store the *attention weights for every time step*.\n",
        "\n",
        "Note: The encoder output is calculated only once for one input."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rgz-TYyqxKJz",
        "outputId": "cb3ad41d-dbb7-461c-eab2-a20eda0a9108"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x7b36fd251120>"
            ]
          },
          "execution_count": 104,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# restoring the latest checkpoint in checkpoint_dir\n",
        "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "id": "xqYcBQWoEU4m"
      },
      "outputs": [],
      "source": [
        "def evaluate(inputs, encoder, decoder, lang, max_length_inp, max_length_targ):\n",
        "\n",
        "    attention_plot = np.zeros((max_length_targ, max_length_inp))\n",
        "    sentence = ''\n",
        "    for i in inputs[0]:\n",
        "        if i == 0:\n",
        "            break\n",
        "        sentence = sentence + lang.idx2word[i] + ' '\n",
        "    sentence = sentence[:-1]\n",
        "\n",
        "    inputs = tf.convert_to_tensor(inputs)\n",
        "\n",
        "    result = ''\n",
        "\n",
        "    hidden = [tf.zeros((1, units))]\n",
        "    enc_out, enc_hidden = encoder(inputs, hidden)\n",
        "\n",
        "    dec_hidden = enc_hidden\n",
        "    dec_input = tf.expand_dims([lang.word2idx['<start>']], 0)\n",
        "\n",
        "    for t in range(max_length_targ):\n",
        "        predictions, dec_hidden, attention_weights = decoder(dec_input, dec_hidden, enc_out)\n",
        "\n",
        "        # storing the attention weights to plot later on\n",
        "        attention_weights = tf.reshape(attention_weights, (-1, ))\n",
        "        attention_plot[t] = attention_weights.numpy()\n",
        "\n",
        "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
        "\n",
        "        result += lang.idx2word[predicted_id] + ' '\n",
        "\n",
        "        if lang.idx2word[predicted_id] == '<end>':\n",
        "            return result, sentence, attention_plot\n",
        "\n",
        "        # the predicted ID is fed back into the model\n",
        "        dec_input = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "    return result, sentence, attention_plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 204,
      "metadata": {
        "id": "AN9VTmmdEXk_"
      },
      "outputs": [],
      "source": [
        "def predict_random_val_sentence():\n",
        "    actual_sent = ''\n",
        "    k = np.random.randint(len(input_tensor_train))\n",
        "    random_input = input_tensor_train[k]\n",
        "    random_output = target_tensor_train[k]\n",
        "    random_input = np.expand_dims(random_input, 0)\n",
        "    result, sentence, attention_plot = evaluate(random_input, encoder, decoder, lang, max_length_inp, max_length_targ)\n",
        "    candidate_translation = result[:-6]  # Exclude start and end tokens\n",
        "    print('Input: {}'.format(sentence[8:-6]))\n",
        "    print('Predicted translation: {}'.format(candidate_translation))\n",
        "    for i in random_output:\n",
        "        if i == 0:\n",
        "            break\n",
        "        actual_sent = actual_sent + lang.idx2word[i] + ' '\n",
        "    actual_sent = actual_sent[8:-7]\n",
        "    print('Actual translation: {}'.format(actual_sent))\n",
        "    reference_translation = actual_sent\n",
        "    attention_plot = attention_plot[:len(result.split(' '))-2, 1:len(sentence.split(' '))-1]\n",
        "    sentence, result = sentence.split(' '), result.split(' ')\n",
        "    sentence = sentence[1:-1]\n",
        "    result = result[:-2]\n",
        "\n",
        "    # Use plotly to generate the heatmap\n",
        "    trace = go.Heatmap(z=attention_plot, x=sentence, y=result, colorscale='greens')\n",
        "    data = [trace]\n",
        "    iplot(data)\n",
        "\n",
        "    return candidate_translation, reference_translation, random_input\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 275,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 594
        },
        "id": "tg_VRQxHobh1",
        "outputId": "9134449e-eadf-4166-da95-5e918e6dd5ed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input: are you free today\n",
            "Predicted translation: you free today \n",
            "Actual translation: you free today\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.24.1.min.js\"></script>                <div id=\"24d28f35-7b0d-441a-9e2c-b44731f9fc06\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"24d28f35-7b0d-441a-9e2c-b44731f9fc06\")) {                    Plotly.newPlot(                        \"24d28f35-7b0d-441a-9e2c-b44731f9fc06\",                        [{\"colorscale\":[[0.0,\"rgb(247,252,245)\"],[0.125,\"rgb(229,245,224)\"],[0.25,\"rgb(199,233,192)\"],[0.375,\"rgb(161,217,155)\"],[0.5,\"rgb(116,196,118)\"],[0.625,\"rgb(65,171,93)\"],[0.75,\"rgb(35,139,69)\"],[0.875,\"rgb(0,109,44)\"],[1.0,\"rgb(0,68,27)\"]],\"x\":[\"are\",\"you\",\"free\",\"today\"],\"y\":[\"you\",\"free\",\"today\"],\"z\":[[0.052067723125219345,0.029907001182436943,0.435197114944458,0.2577938735485077],[0.00022677636297885329,0.00022733589867129922,0.935789942741394,0.06281137466430664],[0.005507738795131445,0.010971647687256336,0.6645426750183105,0.03176691383123398]],\"type\":\"heatmap\"}],                        {\"template\":{\"data\":{\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"choropleth\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"choropleth\"}],\"contourcarpet\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"contourcarpet\"}],\"contour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"contour\"}],\"heatmapgl\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmapgl\"}],\"heatmap\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmap\"}],\"histogram2dcontour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2dcontour\"}],\"histogram2d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2d\"}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"mesh3d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"mesh3d\"}],\"parcoords\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"parcoords\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}],\"scatter3d\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter3d\"}],\"scattercarpet\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattercarpet\"}],\"scattergeo\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergeo\"}],\"scattergl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergl\"}],\"scattermapbox\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattermapbox\"}],\"scatterpolargl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolargl\"}],\"scatterpolar\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolar\"}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"scatterternary\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterternary\"}],\"surface\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"surface\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}]},\"layout\":{\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"autotypenumbers\":\"strict\",\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]],\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]},\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"geo\":{\"bgcolor\":\"white\",\"lakecolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"showlakes\":true,\"showland\":true,\"subunitcolor\":\"white\"},\"hoverlabel\":{\"align\":\"left\"},\"hovermode\":\"closest\",\"mapbox\":{\"style\":\"light\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"ternary\":{\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"title\":{\"x\":0.05},\"xaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2},\"yaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2}}}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('24d28f35-7b0d-441a-9e2c-b44731f9fc06');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "\n",
        "candidate_translation, reference_translation, random_input = predict_random_val_sentence()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 195,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zhXdKahnfbb9",
        "outputId": "0e22a4f7-fd11-4567-e02c-53b49cb37e92"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.800737402916808\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "score = sentence_bleu([reference_translation], candidate_translation, weights=(0.25, 0.25))\n",
        "print(score)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 190,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8bmrAlbe3kMh",
        "outputId": "022c9a8e-dd05-4144-f2a6-dfab358b4430"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predicted:  i do not make any difference to hear that \n",
            "Reference:  i in dilemma what to do\n",
            "\n",
            "Predicted:  do you need something \n",
            "Reference:  you hide something\n",
            "\n",
            "Predicted:  you \n",
            "Reference:  speak softly\n",
            "\n",
            "Predicted:  what you \n",
            "Reference:  what you do\n",
            "\n",
            "Predicted:  you from \n",
            "Reference:  which college school you from\n",
            "\n",
            "Predicted:  you cry \n",
            "Reference:  my name xxxxxxxx\n",
            "\n",
            "Predicted:  what you do not like you do not lik\n",
            "Reference:  what do you want become\n",
            "\n",
            "Predicted:  i trust you \n",
            "Reference:  i need water\n",
            "\n",
            "Predicted:  comb you disappointed \n",
            "Reference:  what your phone number\n",
            "\n",
            "Predicted:  i like you \n",
            "Reference:  i cry\n",
            "\n",
            "Predicted:  you \n",
            "Reference:  congratulatiions\n",
            "\n",
            "BLEU Score: 0.3933172938363534\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "\n",
        "def predict_and_print_validation_set(input_tensor_val, target_tensor_val, encoder, decoder, lang, max_length_inp, max_length_targ):\n",
        "    predicted_translations = []\n",
        "    reference_translations = []\n",
        "\n",
        "    # Predict translations for the entire validation set\n",
        "    for i in range(len(input_tensor_val)):\n",
        "        random_input = np.expand_dims(input_tensor_val[i], 0)\n",
        "        result, _, _ = evaluate(random_input, encoder, decoder, lang, max_length_inp, max_length_targ)\n",
        "        candidate_translation = result[:-6]  # Exclude start and end tokens\n",
        "        predicted_translations.append(candidate_translation)\n",
        "\n",
        "        actual_sent = ''\n",
        "        random_output = target_tensor_val[i]\n",
        "        for idx in random_output:\n",
        "            if idx == 0:\n",
        "                break\n",
        "            actual_sent = actual_sent + lang.idx2word[idx] + ' '\n",
        "        reference_translation = actual_sent[8:-7]  # Exclude start and end tokens\n",
        "        reference_translations.append(reference_translation)\n",
        "\n",
        "    # Print predicted and reference translations\n",
        "    for predicted, reference in zip(predicted_translations, reference_translations):\n",
        "        print(\"Predicted: \", predicted)\n",
        "        print(\"Reference: \", reference)\n",
        "        print()\n",
        "\n",
        "    # Calculate BLEU score\n",
        "    bleu_score = corpus_bleu([[ref.split()] for ref in reference_translations], [pred.split() for pred in predicted_translations], weights=(0.25, 0.25))\n",
        "    print(\"BLEU Score:\", bleu_score)\n",
        "\n",
        "# Usage:\n",
        "predict_and_print_validation_set(input_tensor_val, target_tensor_val, encoder, decoder, lang, max_length_inp, max_length_targ)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
